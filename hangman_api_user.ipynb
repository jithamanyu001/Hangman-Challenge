{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trexquant Interview Project (The Hangman Game)\n",
    "\n",
    "* Copyright Trexquant Investment LP. All Rights Reserved. \n",
    "* Redistribution of this question without written consent from Trexquant is prohibited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instruction:\n",
    "For this coding test, your mission is to write an algorithm that plays the game of Hangman through our API server. \n",
    "\n",
    "When a user plays Hangman, the server first selects a secret word at random from a list. The server then returns a row of underscores (space separated)—one for each letter in the secret word—and asks the user to guess a letter. If the user guesses a letter that is in the word, the word is redisplayed with all instances of that letter shown in the correct positions, along with any letters correctly guessed on previous turns. If the letter does not appear in the word, the user is charged with an incorrect guess. The user keeps guessing letters until either (1) the user has correctly guessed all the letters in the word\n",
    "or (2) the user has made six incorrect guesses.\n",
    "\n",
    "You are required to write a \"guess\" function that takes current word (with underscores) as input and returns a guess letter. You will use the API codes below to play 1,000 Hangman games. You have the opportunity to practice before you want to start recording your game results.\n",
    "\n",
    "Your algorithm is permitted to use a training set of approximately 250,000 dictionary words. Your algorithm will be tested on an entirely disjoint set of 250,000 dictionary words. Please note that this means the words that you will ultimately be tested on do NOT appear in the dictionary that you are given. You are not permitted to use any dictionary other than the training dictionary we provided. This requirement will be strictly enforced by code review.\n",
    "\n",
    "You are provided with a basic, working algorithm. This algorithm will match the provided masked string (e.g. a _ _ l e) to all possible words in the dictionary, tabulate the frequency of letters appearing in these possible words, and then guess the letter with the highest frequency of appearence that has not already been guessed. If there are no remaining words that match then it will default back to the character frequency distribution of the entire dictionary.\n",
    "\n",
    "This benchmark strategy is successful approximately 18% of the time. Your task is to design an algorithm that significantly outperforms this benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn,optim\n",
    "import random\n",
    "import re\n",
    "device=torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Model -- final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(word,stoi=None):\n",
    "    l=[]\n",
    "    if stoi is not None:\n",
    "        for i in word:\n",
    "            l.append(stoi[i])\n",
    "        return l\n",
    "    for i in word:\n",
    "        l.append(ord(i)-97)\n",
    "    return l\n",
    "def decode(tokens,itos):\n",
    "    l=[]\n",
    "    for i in tokens:\n",
    "        l.append(itos[i])\n",
    "    return \"\".join(l)\n",
    "class TextDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,text_file=\"words_250000_train.txt\",start_tok=\"<sos>\",end_tok=\"<eos>\"):\n",
    "        super(TextDataset,self).__init__()\n",
    "        self.data=open(text_file).readlines()\n",
    "        self.data=[ re.sub(\"\\n\",\"\",i) for i in self.data] #i[:-1] to remove the last '\\n'\n",
    "        self.vocab=list(set(\"\".join(self.data)))\n",
    "        self.vocab.sort()\n",
    "        self.itos={i:ch for i,ch in enumerate(self.vocab)}\n",
    "        letters=[ch for ch in self.vocab]\n",
    "        self.vocab.append(\"_\") \n",
    "        self.vocab.append(\"<pad>\")\n",
    "        self.vocab.append(start_tok)\n",
    "        if end_tok not in self.vocab:\n",
    "            self.vocab.append(end_tok)\n",
    "        self.vocab.sort()\n",
    "        self.vocab_size=len(self.vocab)\n",
    "        self.stoi={ch:i for i,ch in enumerate(self.vocab)}\n",
    "        self.X_data=[]\n",
    "        self.Y_data=[]\n",
    "        for word in self.data:\n",
    "            chars=list(set(word))\n",
    "            random.shuffle(chars)\n",
    "            temp=[]\n",
    "            for j in chars:\n",
    "                word=re.sub(j,\"_\",word)\n",
    "                #left_out=[ch for ch in letters if ch not in word]\n",
    "                left_out=[]\n",
    "                for ch in letters:\n",
    "                    if ch not in word:\n",
    "                        left_out.append(ch)\n",
    "                    else:\n",
    "                        left_out.append(\"<pad>\")\n",
    "                tempx=[self.stoi[start_tok],]+encode(word,self.stoi)+[self.stoi[end_tok],]\n",
    "                tempy=torch.tensor(encode(j))\n",
    "                temp.append(tempy)\n",
    "                for i in temp:\n",
    "                    num=random.sample(list(range(5)),1)[0]\n",
    "                    keep_out=random.sample(left_out,min(num,len(left_out)))\n",
    "                    left_out=[ch for ch in left_out if ch not in keep_out]\n",
    "                    remains=[]\n",
    "                    for ch in left_out:\n",
    "                        if ch==\"<pad>\" or ch not in keep_out:\n",
    "                            remains.append(ch)\n",
    "                        elif ch in keep_out:\n",
    "                            remains.append(\"<pad>\")\n",
    "                    self.X_data.append(torch.tensor(tempx+encode(remains,self.stoi)))\n",
    "                    self.Y_data.append(i)\n",
    "    def __len__(self):\n",
    "        return len(self.X_data)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X_data[idx],self.Y_data[idx]\n",
    "    \n",
    "def MakeBatch(X_data,Y_data,batch_size=None):\n",
    "    if batch_size is not None:\n",
    "        idx=random.sample(range(len(X_data)),batch_size)\n",
    "        return [X_data[i] for i in idx],torch.tensor([Y_data[i] for i in idx])\n",
    "    else:\n",
    "        return X_data,torch.tensor([Y_data])\n",
    "    \n",
    "class Network(nn.Module):\n",
    "    def  __init__(self,vocab_size,pad_idx,embedding_dim=300,hidden_dim=50,num_layers=2,bidirectional=False,lin_layers=[],activation=nn.ReLU(),outputs=26)-> None:\n",
    "        super(Network,self).__init__()\n",
    "        self.pad_idx=pad_idx\n",
    "        self.num_layers=num_layers\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.bidirectional=bidirectional\n",
    "        self.Embedding=nn.Embedding(vocab_size,embedding_dim,padding_idx=self.pad_idx)\n",
    "        self._init_weights(self.Embedding)\n",
    "        self.LSTM=nn.LSTM(embedding_dim,hidden_dim,num_layers,bidirectional=bidirectional,dropout=0.4,batch_first=True)\n",
    "        for layer in range(num_layers):\n",
    "            for weight in self.LSTM._all_weights[layer]:\n",
    "                if \"weight\" in weight:\n",
    "                    nn.init.xavier_uniform_(getattr(self.LSTM,weight))\n",
    "                if \"bias\" in weight:\n",
    "                    nn.init.uniform_(getattr(self.LSTM,weight))\n",
    "        self.lin_layers=[]\n",
    "        \n",
    "        if(bidirectional):\n",
    "            #self.ln_norm=nn.LayerNorm(2*hidden_dim*num_layers)\n",
    "            self.convert=nn.Linear(2*hidden_dim*num_layers,lin_layers[0])\n",
    "        else:\n",
    "            #self.ln_norm=nn.LayerNorm(hidden_dim*num_layers)\n",
    "            self.convert=nn.Linear(hidden_dim*num_layers,lin_layers[0])\n",
    "        for i,j in zip(lin_layers,lin_layers[1:]):\n",
    "            self.lin_layers.append(nn.Linear(i,j))\n",
    "        self.lin_layers=nn.ModuleList(self.lin_layers)\n",
    "        self.outputs=nn.Linear(lin_layers[-1],outputs)\n",
    "        self._init_weights(self.outputs)\n",
    "        self.act=activation\n",
    "        #self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight.data)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.kaiming_normal_(module.weight.data)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "    \n",
    "    def forward(self,x,y=None):\n",
    "        x=torch.nn.utils.rnn.pad_sequence(x,batch_first=True,padding_value=self.pad_idx)\n",
    "        x=self.Embedding(x)\n",
    "        o,(h,c)=self.LSTM(x,self.init_hidden(x.shape[0]))\n",
    "        h=h.permute(1,0,2)\n",
    "        x=h.reshape(h.shape[0],h.shape[1]*h.shape[2])\n",
    "        x=self.act(self.convert(x))\n",
    "        for layer in self.lin_layers:\n",
    "            x=layer(x)\n",
    "            x=self.act(x)\n",
    "        x=self.outputs(x)\n",
    "        if y is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            loss=torch.nn.functional.cross_entropy(x,y)\n",
    "        return x,loss\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # even with batch_first = True this remains same as docs\n",
    "        hidden_state = torch.zeros(self.num_layers*2 if self.bidirectional else self.num_layers,batch_size,self.hidden_dim).to(device)\n",
    "        cell_state = torch.zeros(self.num_layers*2 if self.bidirectional else self.num_layers,batch_size,self.hidden_dim).to(device)\n",
    "        return (hidden_state, cell_state)\n",
    "    \n",
    "    def generate(self,x):\n",
    "        logits,_=self(x)\n",
    "        probs=torch.nn.functional.softmax(logits,dim=-1)\n",
    "        idx=torch.multinomial(probs.cpu().detach(),1).item()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "embedding_size=150\n",
    "hidden_dim=300\n",
    "num_layers=3\n",
    "bidirectional=True\n",
    "lin_layers=[150,100]\n",
    "activation=nn.SELU()\n",
    "outputs=26\n",
    "epochs=10000\n",
    "lr=0.001\n",
    "batchSize=3000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need not run the below cells, these were used for training purposes. Model weights have been provided in the zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_data=TextDataset(\"words_250000_train.txt\",\"<sos>\",\"<eos>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7132281"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx=list(range(len(Train_data.X_data)))\n",
    "random.shuffle(idx)\n",
    "Train_data.X_data=[Train_data.X_data[i] for i in idx]\n",
    "Train_data.Y_data=[Train_data.Y_data[i] for i in idx]\n",
    "n=int(0.95*len(idx))\n",
    "TrainX,TrainY=Train_data.X_data[:n],Train_data.Y_data[:n]\n",
    "ValX,ValY=Train_data.X_data[n:],Train_data.Y_data[n:]\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n<batchSize*epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (Embedding): Embedding(30, 150, padding_idx=1)\n",
       "  (LSTM): LSTM(150, 300, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (convert): Linear(in_features=1800, out_features=150, bias=True)\n",
       "  (lin_layers): ModuleList(\n",
       "    (0): Linear(in_features=150, out_features=100, bias=True)\n",
       "  )\n",
       "  (outputs): Linear(in_features=100, out_features=26, bias=True)\n",
       "  (act): SELU()\n",
       ")"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=Network(Train_data.vocab_size,Train_data.stoi[\"<pad>\"],embedding_size,hidden_dim,num_layers,bidirectional,lin_layers,activation,outputs).to(device)\n",
    "optimizer=optim.AdamW(model.parameters(),lr=lr)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for 0/10000 Train loss 3.3772692680358887\n",
      "for 0/10000 Val loss 3.361531972885132\n",
      "for 200/10000 Train loss 2.9426233768463135\n",
      "for 200/10000 Val loss 2.9666292667388916\n",
      "for 400/10000 Train loss 2.8501980304718018\n",
      "for 400/10000 Val loss 2.840642213821411\n",
      "for 600/10000 Train loss 2.770334005355835\n",
      "for 600/10000 Val loss 2.7945733070373535\n",
      "for 800/10000 Train loss 2.7603089809417725\n",
      "for 800/10000 Val loss 2.770395278930664\n",
      "for 1000/10000 Train loss 2.7564454078674316\n",
      "for 1000/10000 Val loss 2.7493200302124023\n",
      "for 1200/10000 Train loss 2.7624056339263916\n",
      "for 1200/10000 Val loss 2.7535722255706787\n",
      "for 1400/10000 Train loss 2.741525411605835\n",
      "for 1400/10000 Val loss 2.738529682159424\n",
      "for 1600/10000 Train loss 2.7229456901550293\n",
      "for 1600/10000 Val loss 2.743687391281128\n",
      "for 1800/10000 Train loss 2.713451385498047\n",
      "for 1800/10000 Val loss 2.6982333660125732\n",
      "for 2000/10000 Train loss 2.7062134742736816\n",
      "for 2000/10000 Val loss 2.7058022022247314\n",
      "for 2200/10000 Train loss 2.7005512714385986\n",
      "for 2200/10000 Val loss 2.684464454650879\n",
      "for 2400/10000 Train loss 2.713365316390991\n",
      "for 2400/10000 Val loss 2.7038393020629883\n",
      "for 2600/10000 Train loss 2.688410758972168\n",
      "for 2600/10000 Val loss 2.6790037155151367\n",
      "for 2800/10000 Train loss 2.697115659713745\n",
      "for 2800/10000 Val loss 2.6925339698791504\n",
      "for 3000/10000 Train loss 2.6788885593414307\n",
      "for 3000/10000 Val loss 2.699336290359497\n",
      "for 3200/10000 Train loss 2.6794261932373047\n",
      "for 3200/10000 Val loss 2.690084218978882\n",
      "for 3400/10000 Train loss 2.675677537918091\n",
      "for 3400/10000 Val loss 2.688255548477173\n",
      "for 3600/10000 Train loss 2.6822445392608643\n",
      "for 3600/10000 Val loss 2.6774158477783203\n",
      "for 3800/10000 Train loss 2.665497303009033\n",
      "for 3800/10000 Val loss 2.6670424938201904\n",
      "for 4000/10000 Train loss 2.6703126430511475\n",
      "for 4000/10000 Val loss 2.6832046508789062\n",
      "for 4200/10000 Train loss 2.6712307929992676\n",
      "for 4200/10000 Val loss 2.695476531982422\n",
      "for 4400/10000 Train loss 2.6621553897857666\n",
      "for 4400/10000 Val loss 2.680649995803833\n",
      "for 4600/10000 Train loss 2.6626994609832764\n",
      "for 4600/10000 Val loss 2.6567440032958984\n",
      "for 4800/10000 Train loss 2.662264347076416\n",
      "for 4800/10000 Val loss 2.6896181106567383\n",
      "for 5000/10000 Train loss 2.660651683807373\n",
      "for 5000/10000 Val loss 2.653810977935791\n",
      "for 5200/10000 Train loss 2.650118827819824\n",
      "for 5200/10000 Val loss 2.66135311126709\n",
      "for 5400/10000 Train loss 2.6646032333374023\n",
      "for 5400/10000 Val loss 2.6627798080444336\n",
      "for 5600/10000 Train loss 2.6371283531188965\n",
      "for 5600/10000 Val loss 2.6640827655792236\n",
      "for 5800/10000 Train loss 2.649873971939087\n",
      "for 5800/10000 Val loss 2.6651504039764404\n",
      "for 6000/10000 Train loss 2.6450624465942383\n",
      "for 6000/10000 Val loss 2.6552953720092773\n",
      "for 6200/10000 Train loss 2.627958059310913\n",
      "for 6200/10000 Val loss 2.6843719482421875\n",
      "for 6400/10000 Train loss 2.639643907546997\n",
      "for 6400/10000 Val loss 2.6369807720184326\n",
      "for 6600/10000 Train loss 2.6616876125335693\n",
      "for 6600/10000 Val loss 2.688323974609375\n",
      "for 6800/10000 Train loss 2.673090696334839\n",
      "for 6800/10000 Val loss 2.660024642944336\n",
      "for 7000/10000 Train loss 2.641890525817871\n",
      "for 7000/10000 Val loss 2.644575595855713\n",
      "for 7200/10000 Train loss 2.6385581493377686\n",
      "for 7200/10000 Val loss 2.6398322582244873\n",
      "for 7400/10000 Train loss 2.6098692417144775\n",
      "for 7400/10000 Val loss 2.655479907989502\n",
      "for 7600/10000 Train loss 2.648746967315674\n",
      "for 7600/10000 Val loss 2.686537027359009\n",
      "for 7800/10000 Train loss 2.632105588912964\n",
      "for 7800/10000 Val loss 2.622802972793579\n",
      "for 8000/10000 Train loss 2.6546590328216553\n",
      "for 8000/10000 Val loss 2.6661229133605957\n",
      "for 8200/10000 Train loss 2.6525092124938965\n",
      "for 8200/10000 Val loss 2.6712448596954346\n",
      "for 8400/10000 Train loss 2.6309635639190674\n",
      "for 8400/10000 Val loss 2.649165630340576\n",
      "for 8600/10000 Train loss 2.6090497970581055\n",
      "for 8600/10000 Val loss 2.6426310539245605\n",
      "for 8800/10000 Train loss 2.6136257648468018\n",
      "for 8800/10000 Val loss 2.675159215927124\n",
      "for 9000/10000 Train loss 2.646793842315674\n",
      "for 9000/10000 Val loss 2.6516716480255127\n",
      "for 9200/10000 Train loss 2.6349809169769287\n",
      "for 9200/10000 Val loss 2.6359055042266846\n",
      "for 9400/10000 Train loss 2.636099338531494\n",
      "for 9400/10000 Val loss 2.679899215698242\n",
      "for 9600/10000 Train loss 2.608930826187134\n",
      "for 9600/10000 Val loss 2.654709577560425\n",
      "for 9800/10000 Train loss 2.637516736984253\n",
      "for 9800/10000 Val loss 2.6485815048217773\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    if epoch%200==0:\n",
    "        x,y=MakeBatch(TrainX,TrainY,batchSize)\n",
    "        x,y=[ele.to(device) for ele in x],y.to(device)\n",
    "        logits,loss=model(x,y)\n",
    "        print(f\"for {epoch}/{epochs} Train loss {loss.cpu().detach().item()}\")\n",
    "        x,y=MakeBatch(ValX,ValY,batchSize)\n",
    "        x,y=[ele.to(device) for ele in x],y.to(device)\n",
    "        logits,loss=model(x,y)\n",
    "        print(f\"for {epoch}/{epochs} Val loss {loss.cpu().detach().item()}\")\n",
    "    optimizer.zero_grad()\n",
    "    x,y=MakeBatch(TrainX,TrainY,batchSize)\n",
    "    x,y=[ele.to(device) for ele in x],y.to(device)\n",
    "    logits,loss=model(x,y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (Embedding): Embedding(30, 150, padding_idx=1)\n",
       "  (LSTM): LSTM(150, 300, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (convert): Linear(in_features=1800, out_features=150, bias=True)\n",
       "  (lin_layers): ModuleList(\n",
       "    (0): Linear(in_features=150, out_features=100, bias=True)\n",
       "  )\n",
       "  (outputs): Linear(in_features=100, out_features=26, bias=True)\n",
       "  (act): SELU()\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do not run the below models they were only put here to show work #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer's Encoder based model #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[0,:x.size(1),:]\n",
    "        return self.dropout(x)\n",
    "def padding_mask(inputs,ins):\n",
    "    mask=torch.ones_like(ins)\n",
    "    for i,ele in enumerate(inputs):\n",
    "        mask[i,:ele.shape[0]]=0\n",
    "    return mask.bool()\n",
    "class TransformersNetwork(nn.Module):\n",
    "    def __init__(self,vocab_size,pad_idx,embedding_dim=300,num_head=5,num_stacks=2,layers=[100,50,26],outputs=26,activation=nn.ReLU()):\n",
    "        super(TransformersNetwork,self).__init__()\n",
    "        self.pad_idx=pad_idx\n",
    "        self.Embedding=nn.Embedding(vocab_size,embedding_dim,padding_idx=self.pad_idx)\n",
    "        self._init_weights(self.Embedding)\n",
    "        self.pos_encoding=PositionalEncoding(embedding_dim)\n",
    "        self.attentions=[nn.MultiheadAttention(embedding_dim,num_head,dropout=0.2,batch_first=True) for _ in range(num_stacks)]\n",
    "        self.attentions=nn.ModuleList(self.attentions)\n",
    "        self.lin_layers=[nn.Sequential(nn.Linear(embedding_dim,embedding_dim*4),nn.GELU(),nn.Linear(embedding_dim*4,embedding_dim)) for _ in range(num_stacks)]\n",
    "        self.lin_layers=nn.ModuleList(self.lin_layers)\n",
    "        for layer in self.lin_layers:\n",
    "            for l in layer:\n",
    "                self._init_weights(l)\n",
    "        self.ln1=nn.LayerNorm(embedding_dim)\n",
    "        self.ln2=nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.final_layers=[]\n",
    "        self.final_layers.append(nn.Linear(embedding_dim,layers[0]))\n",
    "        for i,j in zip(layers,layers[1:]):\n",
    "            self.final_layers.append(nn.Linear(i,j))\n",
    "        self.final_layers=nn.ModuleList(self.final_layers)\n",
    "        self.outputs=nn.Linear(layers[-1],outputs)\n",
    "        self._init_weights(self.outputs)\n",
    "        for layer in self.final_layers:\n",
    "            self._init_weights(layer)\n",
    "        self.activation=activation\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_normal_(module.weight.data)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.xavier_normal_(module.weight.data)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "        \n",
    "    def forward(self,inputs,outputs=None):\n",
    "        ins=nn.utils.rnn.pad_sequence(inputs,padding_value=self.pad_idx,batch_first=True)\n",
    "        embedding=self.Embedding(ins)\n",
    "        pad_mask=padding_mask(inputs,ins)\n",
    "        embedding_pos=self.pos_encoding(embedding)\n",
    "        for attention,lin_layer in zip(self.attentions,self.lin_layers):\n",
    "            atten_outs,_=attention(embedding_pos,embedding_pos,embedding_pos,key_padding_mask=pad_mask)\n",
    "            atten_outs=self.ln1(atten_outs+embedding_pos)\n",
    "            embedding_pos=self.ln2(atten_outs+lin_layer(atten_outs))\n",
    "        \n",
    "        averaged=embedding_pos.sum(dim=1)/(pad_mask==False).float().sum(dim=1,keepdims=True).to(device)\n",
    "        for fin_layer in self.final_layers:\n",
    "            averaged=self.activation(fin_layer(averaged))\n",
    "        logits=self.outputs(averaged)\n",
    "        if outputs is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            loss=nn.functional.cross_entropy(logits,outputs)\n",
    "        \n",
    "        return logits,loss   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM + Attention #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Attention(nn.Module):\n",
    "    def  __init__(self,vocab_size,pad_idx,embedding_dim=300,hidden_dim=50,num_layers=2,bidirectional=False,num_heads=10,num_stacks=3,lin_layers=[],activation=nn.ReLU(),outputs=26)-> None:\n",
    "        super(Network,self).__init__()\n",
    "        self.pad_idx=pad_idx\n",
    "        self.Embedding=nn.Embedding(vocab_size,embedding_dim,padding_idx=self.pad_idx)\n",
    "        self._init_weights(self.Embedding)\n",
    "        self.LSTM=nn.LSTM(embedding_dim,hidden_dim,num_layers,bidirectional=bidirectional,dropout=0.4,batch_first=True)\n",
    "        for layer in range(num_layers):\n",
    "            for weight in self.LSTM._all_weights[layer]:\n",
    "                if \"weight\" in weight:\n",
    "                    nn.init.xavier_uniform_(getattr(self.LSTM,weight))\n",
    "                if \"bias\" in weight:\n",
    "                    nn.init.uniform_(getattr(self.LSTM,weight))\n",
    "        \n",
    "        #self.mha=nn.MultiheadAttention(embedding_dim,num_heads,batch_first=True,dropout=0.2)\n",
    "        \n",
    "        if(bidirectional):\n",
    "        #    self.ln_norm=nn.LayerNorm(2*hidden_dim*num_layers)\n",
    "            self.convert=nn.Linear(2*hidden_dim*num_layers,embedding_dim)\n",
    "        else:\n",
    "        #    self.ln_norm=nn.LayerNorm(hidden_dim*num_layers)\n",
    "            self.convert=nn.Linear(hidden_dim*num_layers,embedding_dim)\n",
    "        self.attentions=[nn.MultiheadAttention(embedding_dim,num_heads,dropout=0.2,batch_first=True) for _ in range(num_stacks)]\n",
    "        self.attentions=nn.ModuleList(self.attentions)\n",
    "        self.lin_layers=[nn.Sequential(nn.Linear(embedding_dim,embedding_dim*4),nn.GELU(),nn.Linear(embedding_dim*4,embedding_dim)) for _ in range(num_stacks)]\n",
    "        self.lin_layers=nn.ModuleList(self.lin_layers)\n",
    "        for layer in self.lin_layers:\n",
    "            for l in layer:\n",
    "                self._init_weights(l)\n",
    "        self.fin_layers=[]\n",
    "        self.fin_layers.append(nn.Linear(embedding_dim,lin_layers[0]))\n",
    "        for i,j in zip(lin_layers,lin_layers[1:]):\n",
    "            self.fin_layers.append(nn.Linear(i,j))\n",
    "        self.fin_layers=nn.ModuleList(self.fin_layers)\n",
    "        self.outputs=nn.Linear(lin_layers[-1],outputs)\n",
    "        self._init_weights(self.outputs)\n",
    "        self.act=activation\n",
    "        self.ln1=nn.LayerNorm(embedding_dim)\n",
    "        self.ln2=nn.LayerNorm(embedding_dim)\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.kaiming_normal_(module.weight.data)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            nn.init.kaiming_normal_(module.weight.data)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "\n",
    "    def forward(self,x,avail,y=None):\n",
    "        x=torch.nn.utils.rnn.pad_sequence(x,batch_first=True,padding_value=self.pad_idx)\n",
    "        x=self.Embedding(x)\n",
    "        o,(h,c)=self.LSTM(x)\n",
    "        h=h.permute(1,0,2)\n",
    "        x=h.reshape(h.shape[0],1,h.shape[1]*h.shape[2])\n",
    "        avail_pad=torch.nn.utils.rnn.pad_sequence(avail,batch_first=True,padding_value=self.pad_idx)\n",
    "        pad_mask=padding_mask(avail,avail_pad)\n",
    "        avail=self.Embedding(avail_pad)\n",
    "        x=self.convert(x)\n",
    "        for attention,lin_layer in zip(self.attentions,self.lin_layers):\n",
    "            atten_outs,_=attention(x,avail,avail)\n",
    "            atten_outs=self.ln1(atten_outs+x)\n",
    "            x=self.ln2(atten_outs+lin_layer(atten_outs))\n",
    "        #atten_outs,atten_weights=self.mha(x,avail,avail)\n",
    "        #print(atten_outs,atten_outs.shape)\n",
    "        #x=self.act(self.convert(torch.squeeze(atten_outs,1)))\n",
    "        x=torch.squeeze(x,1)\n",
    "        #print(x.shape)\n",
    "        for layer in self.fin_layers:\n",
    "            x=layer(x)\n",
    "            x=self.act(x)\n",
    "        x=self.outputs(x)\n",
    "        if y is None:\n",
    "            loss=None\n",
    "        else:\n",
    "            loss=torch.nn.functional.cross_entropy(x,y)\n",
    "        return x,loss\n",
    "    def generate(self,x):\n",
    "        logits,_=self(x)\n",
    "        probs=torch.nn.functional.softmax(logits,dim=-1)\n",
    "        idx=torch.multinomial(probs.cpu().detach(),1).item()\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigram Model #  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiGramModel(nn.Module):\n",
    "  def __init__(self,vocab_size,embed_size) -> None:\n",
    "    super(BiGramModel,self).__init__()\n",
    "    self.embed=nn.Embedding(vocab_size,embed_size)\n",
    "    nn.init.kaiming_normal_(self.embed.weight)\n",
    "    self.lin=nn.Linear(embed_size,vocab_size)\n",
    "    nn.init.kaiming_normal_(self.lin.weight)\n",
    "  def forward(self,idx,target=None):\n",
    "    x=self.embed(idx)\n",
    "    logits=self.lin(x)\n",
    "    if target is None:\n",
    "      loss=None\n",
    "    else:\n",
    "      B,T,C=logits.shape\n",
    "      logs=logits.view(B*T,C)\n",
    "      target=target.view(B*T)\n",
    "      loss=nn.functional.cross_entropy(logs,target)\n",
    "    return logits,loss\n",
    "  def generate(self,idx):\n",
    "    logits,_=self(idx)\n",
    "    probs=nn.functional.softmax(logits[:,-1,:],dim=-1)\n",
    "    idx_next=torch.multinomial(probs,num_samples=1)\n",
    "    return idx_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=open(\"words_250000_train.txt\").readlines()\n",
    "Data=[re.sub(\"\\n\",\"\",word) for word in Data]\n",
    "start=\"0\"\n",
    "end=\"1\"\n",
    "changed_data=[start+word+end for word in Data]\n",
    "vocab=sorted(list(set(\"\".join(changed_data))))\n",
    "Bigram_stoi={a:i for i,a in enumerate(vocab)}\n",
    "Bigram_itos={Bigram_stoi[a]:a for a in Bigram_stoi}\n",
    "f=\"\".join(changed_data)\n",
    "block_size=10\n",
    "batch_size=54\n",
    "embed_size=100\n",
    "vocab_size=len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(word,stoi=None):\n",
    "    l=[]\n",
    "    if stoi is not None:\n",
    "        for i in word:\n",
    "            l.append(stoi[i])\n",
    "        return l\n",
    "    for i in word:\n",
    "        l.append(ord(i)-97)\n",
    "    return l\n",
    "def decode(tokens,itos):\n",
    "    l=[]\n",
    "    for i in tokens:\n",
    "        l.append(itos[i])\n",
    "    return \"\".join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardModel=BiGramModel(vocab_size,embed_size)\n",
    "forwardoptimizer=optim.AdamW(ForwardModel.parameters(),lr=0.001)\n",
    "BackwardModel=BiGramModel(vocab_size,embed_size)\n",
    "backwardoptimizer=optim.AdamW(BackwardModel.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBatchForward(text,block_size,batch_size):\n",
    "  idx=torch.randint(0,len(text)-batch_size,(batch_size,))\n",
    "  x=torch.tensor([encode(text[i:i+block_size],Bigram_stoi) for i in idx ])\n",
    "  y=torch.tensor([encode(text[i+1:i+block_size+1],Bigram_stoi) for i in idx ])\n",
    "  return x,y\n",
    "def makeBatchBackward(text,block_size,batch_size):\n",
    "  idx=torch.randint(0,len(text)-batch_size,(batch_size,))\n",
    "  x=torch.tensor([encode(text[i+1:i+block_size+1],Bigram_stoi) for i in idx ])\n",
    "  y=torch.tensor([encode(text[i:i+block_size],Bigram_stoi) for i in idx ])\n",
    "  return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward model loss: tensor(2.3662, grad_fn=<NllLossBackward0>)\n",
      "Backward model loss: tensor(2.3458, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#training the forwrd model\n",
    "for i in range(10000):\n",
    "  x,y=makeBatchForward(f,block_size,batch_size)\n",
    "  logs,loss=ForwardModel(x,y)\n",
    "  forwardoptimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  forwardoptimizer.step()\n",
    "print(\"Forward model loss:\",loss)\n",
    "#training the backward model\n",
    "for i in range(10000):\n",
    "  x,y=makeBatchBackward(f,block_size,batch_size)\n",
    "  logs,loss=BackwardModel(x,y)\n",
    "  backwardoptimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  backwardoptimizer.step()\n",
    "print(\"Backward model loss:\",loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hangman game #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (Embedding): Embedding(30, 100, padding_idx=1)\n",
      "  (LSTM): LSTM(100, 300, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
      "  (convert): Linear(in_features=1800, out_features=100, bias=True)\n",
      "  (lin_layers): ModuleList()\n",
      "  (outputs): Linear(in_features=100, out_features=26, bias=True)\n",
      "  (act): SELU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "def encode(word,stoi=None):\n",
    "    l=[]\n",
    "    if stoi is not None:\n",
    "        for i in word:\n",
    "            l.append(stoi[i])\n",
    "        return l\n",
    "    for i in word:\n",
    "        l.append(ord(i)-97)\n",
    "    return l\n",
    "def decode(tokens,itos):\n",
    "    l=[]\n",
    "    for i in tokens:\n",
    "        l.append(itos[i])\n",
    "    return \"\".join(l)\n",
    "\n",
    "model=torch.load(\"./MODEL2.pt\").to(device) # The model was locally saved as MODEL2.pt \n",
    "#gdrive link to model: https://drive.google.com/file/d/1xTy_1Ez3-3r7YkfEImOmTO7fksggIhx3/view?usp=sharing\n",
    "\n",
    "print(model)\n",
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.hangman_url = self.determine_hangman_url()\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        full_dictionary_location = \"words_250000_train.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)        \n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        self.letters=['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "        self.current_dictionary = []\n",
    "        #self.forwardmodel=ForwardModel\n",
    "        #self.backwardmodel=BackwardModel\n",
    "        self.model=model\n",
    "        self.stoi=[\"<sos>\",\"<eos>\",\"<pad>\",\"_\"]+self.letters\n",
    "        self.stoi.sort()\n",
    "        self.stoi={ch:i for i,ch in enumerate(self.stoi)}\n",
    "        self.itos={i:ch for i,ch in enumerate(self.letters)}\n",
    "    @staticmethod\n",
    "    def determine_hangman_url():\n",
    "        links = ['https://trexsim.com', 'https://sg.trexsim.com']\n",
    "\n",
    "        data = {link: 0 for link in links}\n",
    "\n",
    "        for link in links:\n",
    "\n",
    "            requests.get(link)\n",
    "\n",
    "            for i in range(10):\n",
    "                s = time.time()\n",
    "                requests.get(link)\n",
    "                data[link] = time.time() - s\n",
    "\n",
    "        link = sorted(data.items(), key=lambda x: x[1])[0][0]\n",
    "        link += '/trexsim/hangman'\n",
    "        return link\n",
    "\n",
    "    def guess(self, word): # word input example: \"_ p p _ e \"\n",
    "        ###############################################\n",
    "        # Replace with your own \"guess\" function here #\n",
    "        ###############################################\n",
    "        \n",
    "        \n",
    "        clean_word=re.sub(\" \",\"\",word)\n",
    "        inputs=[self.stoi[\"<sos>\"]]+encode(clean_word,self.stoi)+[self.stoi[\"<eos>\"]]\n",
    "        order=[]\n",
    "        for ch in self.letters:\n",
    "           if ch in self.guessed_letters or ch in clean_word:\n",
    "               order.append(\"<pad>\")\n",
    "           else:\n",
    "               order.append(ch)\n",
    "        #left_out=[ch for ch in self.letters if ch not in clean_word and ch not in self.guessed_letters]\n",
    "        inputs+=encode(order,self.stoi)\n",
    "        letter=\"\"\n",
    "        logits,_=model(torch.tensor(inputs).view(1,-1).to(device))\n",
    "        order=torch.argsort(logits[0],descending=True).tolist()\n",
    "        logits=logits[0].tolist()\n",
    "        for log in order:\n",
    "           let=decode([log,],self.itos)\n",
    "           if let not in self.guessed_letters:\n",
    "               letter=let\n",
    "               break\n",
    "        \n",
    "        return letter\n",
    "        \n",
    "\n",
    "\n",
    "    ##########################################################\n",
    "    # You'll likely not need to modify any of the code below #\n",
    "    ##########################################################\n",
    "    \n",
    "    def build_dictionary(self, dictionary_file_location):\n",
    "        text_file = open(dictionary_file_location,\"r\")\n",
    "        full_dictionary = text_file.read().splitlines()\n",
    "        text_file.close()\n",
    "        return full_dictionary\n",
    "                \n",
    "    def start_game(self, practice=True, verbose=True):\n",
    "        # reset guessed letters to empty set and current plausible dictionary to the full dictionary\n",
    "        self.guessed_letters = []\n",
    "        self.current_dictionary = self.full_dictionary\n",
    "                         \n",
    "        response = self.request(\"/new_game\", {\"practice\":practice})\n",
    "        if response.get('status')==\"approved\":\n",
    "            game_id = response.get('game_id')\n",
    "            word = response.get('word')\n",
    "            tries_remains = response.get('tries_remains')\n",
    "            if verbose:\n",
    "                print(\"Successfully start a new game! Game ID: {0}. # of tries remaining: {1}. Word: {2}.\".format(game_id, tries_remains, word))\n",
    "            while tries_remains>0:\n",
    "                # get guessed letter from user code\n",
    "                guess_letter = self.guess(word)\n",
    "                    \n",
    "                # append guessed letter to guessed letters field in hangman object\n",
    "                self.guessed_letters.append(guess_letter)\n",
    "                if verbose:\n",
    "                    print(\"Guessing letter: {0}\".format(guess_letter))\n",
    "                    \n",
    "                try:    \n",
    "                    res = self.request(\"/guess_letter\", {\"request\":\"guess_letter\", \"game_id\":game_id, \"letter\":guess_letter})\n",
    "                except HangmanAPIError:\n",
    "                    print('HangmanAPIError exception caught on request.')\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print('Other exception caught on request.')\n",
    "                    raise e\n",
    "               \n",
    "                if verbose:\n",
    "                    print(\"Sever response: {0}\".format(res))\n",
    "                status = res.get('status')\n",
    "                tries_remains = res.get('tries_remains')\n",
    "                if status==\"success\":\n",
    "                    if verbose:\n",
    "                        print(\"Successfully finished game: {0}\".format(game_id))\n",
    "                    return True\n",
    "                elif status==\"failed\":\n",
    "                    reason = res.get('reason', '# of tries exceeded!')\n",
    "                    if verbose:\n",
    "                        print(\"Failed game: {0}. Because of: {1}\".format(game_id, reason))\n",
    "                    return False\n",
    "                elif status==\"ongoing\":\n",
    "                    word = res.get('word')\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Failed to start a new game\")\n",
    "        return status==\"success\"\n",
    "        \n",
    "    def my_status(self):\n",
    "        return self.request(\"/my_status\", {})\n",
    "    \n",
    "    def request(\n",
    "            self, path, args=None, post_args=None, method=None):\n",
    "        if args is None:\n",
    "            args = dict()\n",
    "        if post_args is not None:\n",
    "            method = \"POST\"\n",
    "\n",
    "        # Add `access_token` to post_args or args if it has not already been\n",
    "        # included.\n",
    "        if self.access_token:\n",
    "            # If post_args exists, we assume that args either does not exists\n",
    "            # or it does not need `access_token`.\n",
    "            if post_args and \"access_token\" not in post_args:\n",
    "                post_args[\"access_token\"] = self.access_token\n",
    "            elif \"access_token\" not in args:\n",
    "                args[\"access_token\"] = self.access_token\n",
    "\n",
    "        time.sleep(0.2)\n",
    "\n",
    "        num_retry, time_sleep = 50, 2\n",
    "        for it in range(num_retry):\n",
    "            try:\n",
    "                response = self.session.request(\n",
    "                    method or \"GET\",\n",
    "                    self.hangman_url + path,\n",
    "                    timeout=self.timeout,\n",
    "                    params=args,\n",
    "                    data=post_args,\n",
    "                    verify=False\n",
    "                )\n",
    "                break\n",
    "            except requests.HTTPError as e:\n",
    "                response = json.loads(e.read())\n",
    "                raise HangmanAPIError(response)\n",
    "            except requests.exceptions.SSLError as e:\n",
    "                if it + 1 == num_retry:\n",
    "                    raise\n",
    "                time.sleep(time_sleep)\n",
    "\n",
    "        headers = response.headers\n",
    "        if 'json' in headers['content-type']:\n",
    "            result = response.json()\n",
    "        elif \"access_token\" in parse_qs(response.text):\n",
    "            query_str = parse_qs(response.text)\n",
    "            if \"access_token\" in query_str:\n",
    "                result = {\"access_token\": query_str[\"access_token\"][0]}\n",
    "                if \"expires\" in query_str:\n",
    "                    result[\"expires\"] = query_str[\"expires\"][0]\n",
    "            else:\n",
    "                raise HangmanAPIError(response.json())\n",
    "        else:\n",
    "            raise HangmanAPIError('Maintype was not text, or querystring')\n",
    "\n",
    "        if result and isinstance(result, dict) and result.get(\"error\"):\n",
    "            raise HangmanAPIError(result)\n",
    "        return result\n",
    "    \n",
    "class HangmanAPIError(Exception):\n",
    "    def __init__(self, result):\n",
    "        self.result = result\n",
    "        self.code = None\n",
    "        try:\n",
    "            self.type = result[\"error_code\"]\n",
    "        except (KeyError, TypeError):\n",
    "            self.type = \"\"\n",
    "\n",
    "        try:\n",
    "            self.message = result[\"error_description\"]\n",
    "        except (KeyError, TypeError):\n",
    "            try:\n",
    "                self.message = result[\"error\"][\"message\"]\n",
    "                self.code = result[\"error\"].get(\"code\")\n",
    "                if not self.type:\n",
    "                    self.type = result[\"error\"].get(\"type\", \"\")\n",
    "            except (KeyError, TypeError):\n",
    "                try:\n",
    "                    self.message = result[\"error_msg\"]\n",
    "                except (KeyError, TypeError):\n",
    "                    self.message = result\n",
    "\n",
    "        Exception.__init__(self, self.message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Usage Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To start a new game:\n",
    "1. Make sure you have implemented your own \"guess\" method.\n",
    "2. Use the access_token that we sent you to create your HangmanAPI object. \n",
    "3. Start a game by calling \"start_game\" method.\n",
    "4. If you wish to test your function without being recorded, set \"practice\" parameter to 1.\n",
    "5. Note: You have a rate limit of 20 new games per minute. DO NOT start more than 20 new games within one minute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "api = HangmanAPI(access_token=\"\", timeout=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12750, 5695)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_runs,_,_,prev_successes=api.my_status()\n",
    "prev_runs,prev_successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (Embedding): Embedding(30, 100, padding_idx=1)\n",
       "  (LSTM): LSTM(100, 300, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (convert): Linear(in_features=1800, out_features=100, bias=True)\n",
       "  (lin_layers): ModuleList()\n",
       "  (outputs): Linear(in_features=100, out_features=26, bias=True)\n",
       "  (act): SELU()\n",
       ")"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing practice games:\n",
    "You can use the command below to play up to 100,000 practice games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1 1\n",
      "0.5 1 2\n",
      "0.3333333333333333 1 3\n",
      "0.5 2 4\n",
      "0.6 3 5\n",
      "0.6666666666666666 4 6\n",
      "0.7142857142857143 5 7\n",
      "0.75 6 8\n",
      "0.7777777777777778 7 9\n",
      "0.8 8 10\n",
      "0.8181818181818182 9 11\n",
      "0.75 9 12\n",
      "0.6923076923076923 9 13\n",
      "0.7142857142857143 10 14\n",
      "0.6666666666666666 10 15\n",
      "0.625 10 16\n",
      "0.6470588235294118 11 17\n",
      "0.6666666666666666 12 18\n",
      "0.6842105263157895 13 19\n",
      "0.65 13 20\n",
      "0.6666666666666666 14 21\n",
      "0.6363636363636364 14 22\n",
      "0.6521739130434783 15 23\n",
      "0.625 15 24\n",
      "0.6 15 25\n",
      "0.5769230769230769 15 26\n",
      "0.5555555555555556 15 27\n",
      "0.5357142857142857 15 28\n",
      "0.5517241379310345 16 29\n",
      "0.5666666666666667 17 30\n",
      "0.5483870967741935 17 31\n",
      "0.53125 17 32\n",
      "0.5454545454545454 18 33\n",
      "0.5588235294117647 19 34\n",
      "0.5714285714285714 20 35\n",
      "0.5555555555555556 20 36\n",
      "0.5405405405405406 20 37\n",
      "0.5263157894736842 20 38\n",
      "0.5384615384615384 21 39\n",
      "0.55 22 40\n",
      "0.5365853658536586 22 41\n",
      "0.5238095238095238 22 42\n",
      "0.5348837209302325 23 43\n",
      "0.5227272727272727 23 44\n",
      "0.5333333333333333 24 45\n",
      "0.5217391304347826 24 46\n",
      "0.5319148936170213 25 47\n",
      "0.5416666666666666 26 48\n",
      "0.5306122448979592 26 49\n",
      "0.52 26 50\n",
      "0.5098039215686274 26 51\n",
      "0.5192307692307693 27 52\n",
      "0.5283018867924528 28 53\n",
      "0.5185185185185185 28 54\n",
      "0.509090909090909 28 55\n",
      "0.5178571428571429 29 56\n",
      "0.5263157894736842 30 57\n",
      "0.5344827586206896 31 58\n",
      "0.5423728813559322 32 59\n",
      "0.5333333333333333 32 60\n",
      "0.5245901639344263 32 61\n",
      "0.5161290322580645 32 62\n",
      "0.5079365079365079 32 63\n",
      "0.5 32 64\n",
      "0.49230769230769234 32 65\n",
      "0.5 33 66\n",
      "0.4925373134328358 33 67\n",
      "0.5 34 68\n",
      "0.5072463768115942 35 69\n",
      "0.5 35 70\n",
      "0.5070422535211268 36 71\n",
      "0.5 36 72\n",
      "0.4931506849315068 36 73\n",
      "0.5 37 74\n",
      "0.49333333333333335 37 75\n",
      "0.5 38 76\n",
      "0.5064935064935064 39 77\n",
      "0.5128205128205128 40 78\n",
      "0.5189873417721519 41 79\n",
      "0.525 42 80\n",
      "0.5308641975308642 43 81\n",
      "0.5365853658536586 44 82\n",
      "0.5301204819277109 44 83\n",
      "0.5238095238095238 44 84\n",
      "0.5176470588235295 44 85\n",
      "0.5116279069767442 44 86\n",
      "0.5172413793103449 45 87\n",
      "0.5113636363636364 45 88\n",
      "0.5056179775280899 45 89\n",
      "0.5111111111111111 46 90\n",
      "0.5164835164835165 47 91\n",
      "0.5217391304347826 48 92\n",
      "0.5161290322580645 48 93\n",
      "0.5106382978723404 48 94\n",
      "0.5157894736842106 49 95\n",
      "0.5208333333333334 50 96\n",
      "0.5257731958762887 51 97\n",
      "0.5306122448979592 52 98\n",
      "0.5252525252525253 52 99\n",
      "0.53 53 100\n",
      "0.5247524752475248 53 101\n",
      "0.5196078431372549 53 102\n",
      "0.5242718446601942 54 103\n",
      "0.5288461538461539 55 104\n",
      "0.5333333333333333 56 105\n",
      "0.5377358490566038 57 106\n",
      "0.5420560747663551 58 107\n",
      "0.5462962962962963 59 108\n",
      "0.5504587155963303 60 109\n",
      "0.5545454545454546 61 110\n",
      "0.5585585585585585 62 111\n",
      "0.5535714285714286 62 112\n",
      "0.5575221238938053 63 113\n",
      "0.5526315789473685 63 114\n",
      "0.5478260869565217 63 115\n",
      "0.5431034482758621 63 116\n",
      "0.5470085470085471 64 117\n",
      "0.5508474576271186 65 118\n",
      "0.5462184873949579 65 119\n",
      "0.55 66 120\n",
      "0.5537190082644629 67 121\n",
      "0.5573770491803278 68 122\n",
      "0.5528455284552846 68 123\n",
      "0.5483870967741935 68 124\n",
      "0.544 68 125\n",
      "0.5476190476190477 69 126\n",
      "0.5433070866141733 69 127\n",
      "0.546875 70 128\n",
      "0.5503875968992248 71 129\n",
      "0.5461538461538461 71 130\n",
      "0.5419847328244275 71 131\n",
      "0.5454545454545454 72 132\n",
      "0.5488721804511278 73 133\n",
      "0.5522388059701493 74 134\n",
      "0.5555555555555556 75 135\n",
      "0.5588235294117647 76 136\n",
      "0.5547445255474452 76 137\n",
      "0.5579710144927537 77 138\n",
      "0.5611510791366906 78 139\n",
      "0.5571428571428572 78 140\n",
      "0.5531914893617021 78 141\n",
      "0.5563380281690141 79 142\n",
      "0.5594405594405595 80 143\n",
      "0.5555555555555556 80 144\n",
      "0.5517241379310345 80 145\n",
      "0.5547945205479452 81 146\n",
      "0.5510204081632653 81 147\n",
      "0.5472972972972973 81 148\n",
      "0.5503355704697986 82 149\n",
      "0.5533333333333333 83 150\n",
      "0.5496688741721855 83 151\n",
      "0.5526315789473685 84 152\n",
      "0.5490196078431373 84 153\n",
      "0.5454545454545454 84 154\n",
      "0.5419354838709678 84 155\n",
      "0.5384615384615384 84 156\n",
      "0.5414012738853503 85 157\n",
      "0.5379746835443038 85 158\n",
      "0.5345911949685535 85 159\n",
      "0.53125 85 160\n",
      "0.5341614906832298 86 161\n",
      "0.5308641975308642 86 162\n",
      "0.5276073619631901 86 163\n",
      "0.524390243902439 86 164\n",
      "0.5272727272727272 87 165\n",
      "0.5301204819277109 88 166\n",
      "0.5329341317365269 89 167\n",
      "0.5297619047619048 89 168\n",
      "0.5266272189349113 89 169\n",
      "0.5294117647058824 90 170\n",
      "0.5263157894736842 90 171\n",
      "0.5232558139534884 90 172\n",
      "0.5202312138728323 90 173\n",
      "0.5229885057471264 91 174\n",
      "0.5257142857142857 92 175\n",
      "0.5284090909090909 93 176\n",
      "0.5310734463276836 94 177\n",
      "0.5337078651685393 95 178\n",
      "0.5307262569832403 95 179\n",
      "0.5277777777777778 95 180\n",
      "0.5248618784530387 95 181\n",
      "0.521978021978022 95 182\n",
      "0.5191256830601093 95 183\n",
      "0.5217391304347826 96 184\n",
      "0.518918918918919 96 185\n",
      "0.521505376344086 97 186\n",
      "0.5187165775401069 97 187\n",
      "0.5212765957446809 98 188\n",
      "0.5185185185185185 98 189\n",
      "0.5157894736842106 98 190\n",
      "0.5130890052356021 98 191\n",
      "0.515625 99 192\n",
      "0.5129533678756477 99 193\n",
      "0.5103092783505154 99 194\n",
      "0.5128205128205128 100 195\n",
      "0.5102040816326531 100 196\n",
      "0.5076142131979695 100 197\n",
      "0.51010101010101 101 198\n",
      "0.5125628140703518 102 199\n",
      "0.51 102 200\n",
      "0.5124378109452736 103 201\n",
      "0.5148514851485149 104 202\n",
      "0.5123152709359606 104 203\n",
      "0.5098039215686274 104 204\n",
      "0.5121951219512195 105 205\n",
      "0.5097087378640777 105 206\n",
      "0.5120772946859904 106 207\n",
      "0.5144230769230769 107 208\n",
      "0.5119617224880383 107 209\n",
      "0.5095238095238095 107 210\n",
      "0.5118483412322274 108 211\n",
      "0.5094339622641509 108 212\n",
      "0.5070422535211268 108 213\n",
      "0.5093457943925234 109 214\n",
      "0.5116279069767442 110 215\n",
      "0.5092592592592593 110 216\n",
      "0.511520737327189 111 217\n",
      "0.5091743119266054 111 218\n",
      "0.5114155251141552 112 219\n",
      "0.509090909090909 112 220\n",
      "0.5113122171945701 113 221\n",
      "0.509009009009009 113 222\n",
      "0.5067264573991032 113 223\n",
      "0.5089285714285714 114 224\n",
      "0.5066666666666667 114 225\n",
      "0.504424778761062 114 226\n",
      "0.5022026431718062 114 227\n",
      "0.5 114 228\n",
      "0.5021834061135371 115 229\n",
      "0.5 115 230\n",
      "0.49783549783549785 115 231\n",
      "0.4956896551724138 115 232\n",
      "0.49356223175965663 115 233\n",
      "0.49145299145299143 115 234\n",
      "0.49361702127659574 116 235\n",
      "0.4957627118644068 117 236\n",
      "0.4978902953586498 118 237\n",
      "0.4957983193277311 118 238\n",
      "0.497907949790795 119 239\n",
      "0.5 120 240\n",
      "0.4979253112033195 120 241\n",
      "0.5 121 242\n",
      "0.5020576131687243 122 243\n",
      "0.5040983606557377 123 244\n",
      "0.5061224489795918 124 245\n",
      "0.5040650406504065 124 246\n",
      "0.5020242914979757 124 247\n",
      "0.5 124 248\n",
      "0.5020080321285141 125 249\n",
      "0.504 126 250\n",
      "0.50199203187251 126 251\n",
      "0.5 126 252\n",
      "0.5019762845849802 127 253\n",
      "0.5039370078740157 128 254\n",
      "0.5019607843137255 128 255\n",
      "0.50390625 129 256\n",
      "0.5058365758754864 130 257\n",
      "0.5077519379844961 131 258\n",
      "0.5057915057915058 131 259\n",
      "0.5076923076923077 132 260\n",
      "0.5095785440613027 133 261\n",
      "0.5076335877862596 133 262\n",
      "0.5095057034220533 134 263\n",
      "0.5075757575757576 134 264\n",
      "0.5056603773584906 134 265\n",
      "0.5075187969924813 135 266\n",
      "0.5056179775280899 135 267\n",
      "0.5074626865671642 136 268\n",
      "0.5092936802973977 137 269\n",
      "0.5111111111111111 138 270\n",
      "0.5092250922509225 138 271\n",
      "0.5110294117647058 139 272\n",
      "0.5128205128205128 140 273\n",
      "0.5109489051094891 140 274\n",
      "0.5127272727272727 141 275\n",
      "0.5108695652173914 141 276\n",
      "0.5090252707581228 141 277\n",
      "0.5107913669064749 142 278\n",
      "0.5125448028673835 143 279\n",
      "0.5107142857142857 143 280\n",
      "0.5124555160142349 144 281\n",
      "0.5106382978723404 144 282\n",
      "0.508833922261484 144 283\n",
      "0.5070422535211268 144 284\n",
      "0.5052631578947369 144 285\n",
      "0.5034965034965035 144 286\n",
      "0.5017421602787456 144 287\n",
      "0.5034722222222222 145 288\n",
      "0.5017301038062284 145 289\n",
      "0.503448275862069 146 290\n",
      "0.5017182130584192 146 291\n",
      "0.5 146 292\n",
      "0.5017064846416383 147 293\n",
      "0.5 147 294\n",
      "0.5016949152542373 148 295\n",
      "0.5033783783783784 149 296\n",
      "0.5016835016835017 149 297\n",
      "0.5033557046979866 150 298\n",
      "0.5050167224080268 151 299\n",
      "0.5066666666666667 152 300\n",
      "0.5049833887043189 152 301\n",
      "0.5066225165562914 153 302\n",
      "0.504950495049505 153 303\n",
      "0.5032894736842105 153 304\n",
      "0.5016393442622951 153 305\n",
      "0.5 153 306\n",
      "0.501628664495114 154 307\n",
      "0.5032467532467533 155 308\n",
      "0.5048543689320388 156 309\n",
      "0.5064516129032258 157 310\n",
      "0.5080385852090032 158 311\n",
      "0.5064102564102564 158 312\n",
      "0.5047923322683706 158 313\n",
      "0.5031847133757962 158 314\n",
      "0.5015873015873016 158 315\n",
      "0.5 158 316\n",
      "0.501577287066246 159 317\n",
      "0.5031446540880503 160 318\n",
      "0.5015673981191222 160 319\n",
      "0.503125 161 320\n",
      "0.5015576323987538 161 321\n",
      "0.5 161 322\n",
      "0.4984520123839009 161 323\n",
      "0.5 162 324\n",
      "0.5015384615384615 163 325\n",
      "0.5 163 326\n",
      "0.5015290519877675 164 327\n",
      "0.5 164 328\n",
      "0.49848024316109424 164 329\n",
      "0.5 165 330\n",
      "0.4984894259818731 165 331\n",
      "0.5 166 332\n",
      "0.4984984984984985 166 333\n",
      "0.49700598802395207 166 334\n",
      "0.4955223880597015 166 335\n",
      "0.49702380952380953 167 336\n",
      "0.49554896142433236 167 337\n",
      "0.4940828402366864 167 338\n",
      "0.49557522123893805 168 339\n",
      "0.4970588235294118 169 340\n",
      "0.49853372434017595 170 341\n",
      "0.49707602339181284 170 342\n",
      "0.4956268221574344 170 343\n",
      "0.4941860465116279 170 344\n",
      "0.4956521739130435 171 345\n",
      "0.49421965317919075 171 346\n",
      "0.4956772334293948 172 347\n",
      "0.4942528735632184 172 348\n",
      "0.49570200573065903 173 349\n",
      "0.49714285714285716 174 350\n",
      "0.49572649572649574 174 351\n",
      "0.4971590909090909 175 352\n",
      "0.4985835694050991 176 353\n",
      "0.5 177 354\n",
      "0.5014084507042254 178 355\n",
      "0.5028089887640449 179 356\n",
      "0.5014005602240896 179 357\n",
      "0.5 179 358\n",
      "0.5013927576601671 180 359\n",
      "0.5 180 360\n",
      "0.4986149584487535 180 361\n",
      "0.4972375690607735 180 362\n",
      "0.49586776859504134 180 363\n",
      "0.49725274725274726 181 364\n",
      "0.4986301369863014 182 365\n",
      "0.4972677595628415 182 366\n",
      "0.49591280653950953 182 367\n",
      "0.49728260869565216 183 368\n",
      "0.4959349593495935 183 369\n",
      "0.4972972972972973 184 370\n",
      "0.49595687331536387 184 371\n",
      "0.49731182795698925 185 372\n",
      "0.49865951742627346 186 373\n",
      "0.5 187 374\n",
      "0.49866666666666665 187 375\n",
      "0.4973404255319149 187 376\n",
      "0.4986737400530504 188 377\n",
      "0.5 189 378\n",
      "0.49868073878627966 189 379\n",
      "0.49736842105263157 189 380\n",
      "0.49868766404199477 190 381\n",
      "0.5 191 382\n",
      "0.5013054830287206 192 383\n",
      "0.5 192 384\n",
      "0.4987012987012987 192 385\n",
      "0.5 193 386\n",
      "0.49870801033591733 193 387\n",
      "0.49742268041237114 193 388\n",
      "0.4961439588688946 193 389\n",
      "0.49743589743589745 194 390\n",
      "0.49872122762148335 195 391\n",
      "0.49744897959183676 195 392\n",
      "0.4961832061068702 195 393\n",
      "0.4949238578680203 195 394\n",
      "0.4962025316455696 196 395\n",
      "0.49747474747474746 197 396\n",
      "0.4987405541561713 198 397\n",
      "0.5 199 398\n",
      "0.49874686716791977 199 399\n",
      "0.4975 199 400\n",
      "0.49875311720698257 200 401\n",
      "0.5 201 402\n",
      "0.4987593052109181 201 403\n",
      "0.5 202 404\n",
      "0.5012345679012346 203 405\n",
      "0.5024630541871922 204 406\n",
      "0.5012285012285013 204 407\n",
      "0.5024509803921569 205 408\n",
      "0.5036674816625917 206 409\n",
      "0.5048780487804878 207 410\n",
      "0.5036496350364964 207 411\n",
      "0.5048543689320388 208 412\n",
      "0.5060532687651331 209 413\n",
      "0.5072463768115942 210 414\n",
      "0.5084337349397591 211 415\n",
      "0.5096153846153846 212 416\n",
      "0.5107913669064749 213 417\n",
      "0.5095693779904307 213 418\n",
      "0.5083532219570406 213 419\n",
      "0.5071428571428571 213 420\n",
      "0.505938242280285 213 421\n",
      "0.5071090047393365 214 422\n",
      "0.508274231678487 215 423\n",
      "0.5094339622641509 216 424\n",
      "0.508235294117647 216 425\n",
      "0.5070422535211268 216 426\n",
      "0.5058548009367682 216 427\n",
      "0.5070093457943925 217 428\n",
      "0.5058275058275058 217 429\n",
      "0.5046511627906977 217 430\n",
      "0.5034802784222738 217 431\n",
      "0.5046296296296297 218 432\n",
      "0.5034642032332564 218 433\n",
      "0.5046082949308756 219 434\n",
      "0.503448275862069 219 435\n",
      "0.5022935779816514 219 436\n",
      "0.5011441647597255 219 437\n",
      "0.502283105022831 220 438\n",
      "0.5034168564920274 221 439\n",
      "0.5022727272727273 221 440\n",
      "0.5034013605442177 222 441\n",
      "0.502262443438914 222 442\n",
      "0.5033860045146726 223 443\n",
      "0.5022522522522522 223 444\n",
      "0.503370786516854 224 445\n",
      "0.5044843049327354 225 446\n",
      "0.5055928411633109 226 447\n",
      "0.5044642857142857 226 448\n",
      "0.5033407572383074 226 449\n",
      "0.5044444444444445 227 450\n",
      "0.5055432372505543 228 451\n",
      "0.5066371681415929 229 452\n",
      "0.5077262693156733 230 453\n",
      "0.5088105726872246 231 454\n",
      "0.5098901098901099 232 455\n",
      "0.5109649122807017 233 456\n",
      "0.5120350109409191 234 457\n",
      "0.5109170305676856 234 458\n",
      "0.5098039215686274 234 459\n",
      "0.5108695652173914 235 460\n",
      "0.5097613882863341 235 461\n",
      "0.5108225108225108 236 462\n",
      "0.5118790496760259 237 463\n",
      "0.5129310344827587 238 464\n",
      "0.5118279569892473 238 465\n",
      "0.5128755364806867 239 466\n",
      "0.5117773019271948 239 467\n",
      "0.5106837606837606 239 468\n",
      "0.509594882729211 239 469\n",
      "0.5106382978723404 240 470\n",
      "0.5095541401273885 240 471\n",
      "0.5084745762711864 240 472\n",
      "0.507399577167019 240 473\n",
      "0.5063291139240507 240 474\n",
      "0.5073684210526316 241 475\n",
      "0.5063025210084033 241 476\n",
      "0.5073375262054507 242 477\n",
      "0.5062761506276151 242 478\n",
      "0.5073068893528184 243 479\n",
      "0.50625 243 480\n",
      "0.5051975051975052 243 481\n",
      "0.5062240663900415 244 482\n",
      "0.505175983436853 244 483\n",
      "0.5041322314049587 244 484\n",
      "0.5051546391752577 245 485\n",
      "0.5061728395061729 246 486\n",
      "0.5051334702258727 246 487\n",
      "0.5061475409836066 247 488\n",
      "0.5071574642126789 248 489\n",
      "0.5081632653061224 249 490\n",
      "0.5091649694501018 250 491\n",
      "0.508130081300813 250 492\n",
      "0.5091277890466531 251 493\n",
      "0.5080971659919028 251 494\n",
      "0.509090909090909 252 495\n",
      "0.5080645161290323 252 496\n",
      "0.5090543259557344 253 497\n",
      "0.5080321285140562 253 498\n",
      "0.5090180360721442 254 499\n",
      "0.51 255 500\n",
      "0.5089820359281437 255 501\n",
      "0.5079681274900398 255 502\n",
      "0.5069582504970179 255 503\n",
      "0.5059523809523809 255 504\n",
      "0.5069306930693069 256 505\n",
      "0.5059288537549407 256 506\n",
      "0.504930966469428 256 507\n",
      "0.5039370078740157 256 508\n",
      "0.5049115913555993 257 509\n",
      "0.503921568627451 257 510\n",
      "0.50293542074364 257 511\n",
      "0.501953125 257 512\n",
      "0.5029239766081871 258 513\n",
      "0.5038910505836576 259 514\n",
      "0.5029126213592233 259 515\n",
      "0.5038759689922481 260 516\n",
      "0.504835589941973 261 517\n",
      "0.5057915057915058 262 518\n",
      "0.5048169556840078 262 519\n",
      "0.5057692307692307 263 520\n",
      "0.5067178502879078 264 521\n",
      "0.5057471264367817 264 522\n",
      "0.5047801147227533 264 523\n",
      "0.5038167938931297 264 524\n",
      "0.5047619047619047 265 525\n",
      "0.5057034220532319 266 526\n",
      "0.5066413662239089 267 527\n",
      "0.5075757575757576 268 528\n",
      "0.5066162570888468 268 529\n",
      "0.5056603773584906 268 530\n",
      "0.504708097928437 268 531\n",
      "0.5056390977443609 269 532\n",
      "0.5046904315196998 269 533\n",
      "0.5037453183520599 269 534\n",
      "0.5046728971962616 270 535\n",
      "0.5055970149253731 271 536\n",
      "0.5065176908752328 272 537\n",
      "0.5055762081784386 272 538\n",
      "0.5064935064935064 273 539\n",
      "0.5055555555555555 273 540\n",
      "0.5046210720887245 273 541\n",
      "0.5055350553505535 274 542\n",
      "0.5046040515653776 274 543\n",
      "0.5036764705882353 274 544\n",
      "0.5045871559633027 275 545\n",
      "0.5054945054945055 276 546\n",
      "0.5045703839122486 276 547\n",
      "0.5036496350364964 276 548\n",
      "0.5027322404371585 276 549\n",
      "0.5036363636363637 277 550\n",
      "0.5045372050816697 278 551\n",
      "0.5054347826086957 279 552\n",
      "0.5045207956600362 279 553\n",
      "0.5036101083032491 279 554\n",
      "0.5027027027027027 279 555\n",
      "0.5035971223021583 280 556\n",
      "0.5044883303411131 281 557\n",
      "0.5053763440860215 282 558\n",
      "0.5062611806797853 283 559\n",
      "0.5071428571428571 284 560\n",
      "0.5080213903743316 285 561\n",
      "0.5071174377224199 285 562\n",
      "0.5079928952042628 286 563\n",
      "0.5070921985815603 286 564\n",
      "0.5079646017699115 287 565\n",
      "0.508833922261484 288 566\n",
      "0.5079365079365079 288 567\n",
      "0.5088028169014085 289 568\n",
      "0.507908611599297 289 569\n",
      "0.5070175438596491 289 570\n",
      "0.5061295971978984 289 571\n",
      "0.506993006993007 290 572\n",
      "0.506108202443281 290 573\n",
      "0.5069686411149826 291 574\n",
      "0.5078260869565218 292 575\n",
      "0.5086805555555556 293 576\n",
      "0.5095320623916811 294 577\n",
      "0.5086505190311419 294 578\n",
      "0.5077720207253886 294 579\n",
      "0.5086206896551724 295 580\n",
      "0.5077452667814114 295 581\n",
      "0.506872852233677 295 582\n",
      "0.5077186963979416 296 583\n",
      "0.5068493150684932 296 584\n",
      "0.5076923076923077 297 585\n",
      "0.5068259385665529 297 586\n",
      "0.5059625212947189 297 587\n",
      "0.5051020408163265 297 588\n",
      "0.5042444821731749 297 589\n",
      "0.5050847457627119 298 590\n",
      "0.5042301184433164 298 591\n",
      "0.5033783783783784 298 592\n",
      "0.5042158516020236 299 593\n",
      "0.5033670033670034 299 594\n",
      "0.5042016806722689 300 595\n",
      "0.5033557046979866 300 596\n",
      "0.5041876046901173 301 597\n",
      "0.5050167224080268 302 598\n",
      "0.5041736227045075 302 599\n",
      "0.505 303 600\n",
      "0.5041597337770383 303 601\n",
      "0.5033222591362126 303 602\n",
      "0.5024875621890548 303 603\n",
      "0.5033112582781457 304 604\n",
      "0.5041322314049587 305 605\n",
      "0.504950495049505 306 606\n",
      "0.5057660626029654 307 607\n",
      "0.506578947368421 308 608\n",
      "0.5073891625615764 309 609\n",
      "0.5081967213114754 310 610\n",
      "0.5073649754500819 310 611\n",
      "0.5065359477124183 310 612\n",
      "0.5057096247960848 310 613\n",
      "0.504885993485342 310 614\n",
      "0.5056910569105691 311 615\n",
      "0.5048701298701299 311 616\n",
      "0.5056726094003241 312 617\n",
      "0.5048543689320388 312 618\n",
      "0.5040387722132472 312 619\n",
      "0.5032258064516129 312 620\n",
      "0.5040257648953301 313 621\n",
      "0.5048231511254019 314 622\n",
      "0.5056179775280899 315 623\n",
      "0.5048076923076923 315 624\n",
      "0.504 315 625\n",
      "0.5047923322683706 316 626\n",
      "0.5055821371610846 317 627\n",
      "0.5063694267515924 318 628\n",
      "0.505564387917329 318 629\n",
      "0.5063492063492063 319 630\n",
      "0.5071315372424723 320 631\n",
      "0.5063291139240507 320 632\n",
      "0.5071090047393365 321 633\n",
      "0.5063091482649842 321 634\n",
      "0.5070866141732283 322 635\n",
      "0.5062893081761006 322 636\n",
      "0.5070643642072213 323 637\n",
      "0.5062695924764891 323 638\n",
      "0.5054773082942097 323 639\n",
      "0.5046875 323 640\n",
      "0.5039001560062403 323 641\n",
      "0.5046728971962616 324 642\n",
      "0.5038880248833593 324 643\n",
      "0.5031055900621118 324 644\n",
      "0.5023255813953489 324 645\n",
      "0.5015479876160991 324 646\n",
      "0.500772797527048 324 647\n",
      "0.5 324 648\n",
      "0.49922958397534667 324 649\n",
      "0.5 325 650\n",
      "0.500768049155146 326 651\n",
      "0.5015337423312883 327 652\n",
      "0.5007656967840735 327 653\n",
      "0.5015290519877675 328 654\n",
      "0.5022900763358779 329 655\n",
      "0.5015243902439024 329 656\n",
      "0.502283105022831 330 657\n",
      "0.5030395136778115 331 658\n",
      "0.5022761760242792 331 659\n",
      "0.5015151515151515 331 660\n",
      "0.5007564296520424 331 661\n",
      "0.5 331 662\n",
      "0.49924585218702866 331 663\n",
      "0.49849397590361444 331 664\n",
      "0.4992481203007519 332 665\n",
      "0.5 333 666\n",
      "0.4992503748125937 333 667\n",
      "0.5 334 668\n",
      "0.4992526158445441 334 669\n",
      "0.5 335 670\n",
      "0.5007451564828614 336 671\n",
      "0.5 336 672\n",
      "0.5007429420505201 337 673\n",
      "0.5 337 674\n",
      "0.5007407407407407 338 675\n",
      "0.5014792899408284 339 676\n",
      "0.5007385524372231 339 677\n",
      "0.5014749262536873 340 678\n",
      "0.5007363770250368 340 679\n",
      "0.5 340 680\n",
      "0.49926578560939794 340 681\n",
      "0.49853372434017595 340 682\n",
      "0.4978038067349927 340 683\n",
      "0.49707602339181284 340 684\n",
      "0.49635036496350365 340 685\n",
      "0.4956268221574344 340 686\n",
      "0.4963609898107715 341 687\n",
      "0.49709302325581395 342 688\n",
      "0.49782293178519593 343 689\n",
      "0.4985507246376812 344 690\n",
      "0.49782923299565845 344 691\n",
      "0.4985549132947977 345 692\n",
      "0.49927849927849927 346 693\n",
      "0.5 347 694\n",
      "0.5007194244604316 348 695\n",
      "0.5014367816091954 349 696\n",
      "0.5007173601147776 349 697\n",
      "0.5 349 698\n",
      "0.5007153075822603 350 699\n",
      "0.5014285714285714 351 700\n",
      "0.5007132667617689 351 701\n",
      "0.5 351 702\n",
      "0.5007112375533428 352 703\n",
      "0.5014204545454546 353 704\n",
      "0.502127659574468 354 705\n",
      "0.5028328611898017 355 706\n",
      "0.5035360678925035 356 707\n",
      "0.5028248587570622 356 708\n",
      "0.5021156558533145 356 709\n",
      "0.5014084507042254 356 710\n",
      "0.5021097046413502 357 711\n",
      "0.5014044943820225 357 712\n",
      "0.5007012622720898 357 713\n",
      "0.5 357 714\n",
      "0.5006993006993007 358 715\n",
      "0.5 358 716\n",
      "0.500697350069735 359 717\n",
      "0.5 359 718\n",
      "0.5006954102920723 360 719\n",
      "0.5013888888888889 361 720\n",
      "0.5020804438280166 362 721\n",
      "0.5013850415512465 362 722\n",
      "0.5020746887966805 363 723\n",
      "0.5013812154696132 363 724\n",
      "0.5006896551724138 363 725\n",
      "0.5 363 726\n",
      "0.49931224209078406 363 727\n",
      "0.49862637362637363 363 728\n",
      "0.4993141289437586 364 729\n",
      "0.4986301369863014 364 730\n",
      "0.4993160054719562 365 731\n",
      "0.49863387978142076 365 732\n",
      "0.49931787175989084 366 733\n",
      "0.5 367 734\n",
      "0.5006802721088436 368 735\n",
      "0.5 368 736\n",
      "0.5006784260515604 369 737\n",
      "0.5 369 738\n",
      "0.4993234100135318 369 739\n",
      "0.5 370 740\n",
      "0.4993252361673414 370 741\n",
      "0.5 371 742\n",
      "0.4993270524899058 371 743\n",
      "0.4986559139784946 371 744\n",
      "0.49798657718120803 371 745\n",
      "0.4973190348525469 371 746\n",
      "0.4966532797858099 371 747\n",
      "0.49732620320855614 372 748\n",
      "0.4979973297730307 373 749\n",
      "0.49733333333333335 373 750\n",
      "0.4980026631158455 374 751\n",
      "0.4973404255319149 374 752\n",
      "0.49667994687915007 374 753\n",
      "0.4960212201591512 374 754\n",
      "0.4966887417218543 375 755\n",
      "0.4973544973544973 376 756\n",
      "0.49801849405548215 377 757\n",
      "0.49868073878627966 378 758\n",
      "0.4980237154150198 378 759\n",
      "0.49736842105263157 378 760\n",
      "0.4980289093298292 379 761\n",
      "0.4973753280839895 379 762\n",
      "0.4980340760157274 380 763\n",
      "0.4973821989528796 380 764\n",
      "0.49673202614379086 380 765\n",
      "0.4960835509138381 380 766\n",
      "0.4954367666232073 380 767\n",
      "0.49609375 381 768\n",
      "0.4967490247074122 382 769\n",
      "0.4961038961038961 382 770\n",
      "0.49546044098573283 382 771\n",
      "0.4961139896373057 383 772\n",
      "0.49547218628719275 383 773\n",
      "0.49612403100775193 384 774\n",
      "0.49548387096774194 384 775\n",
      "0.4948453608247423 384 776\n",
      "0.4942084942084942 384 777\n",
      "0.4948586118251928 385 778\n",
      "0.4942233632862644 385 779\n",
      "0.4935897435897436 385 780\n",
      "0.49295774647887325 385 781\n",
      "0.49232736572890023 385 782\n",
      "0.4929757343550447 386 783\n",
      "0.49362244897959184 387 784\n",
      "0.4929936305732484 387 785\n",
      "0.49363867684478374 388 786\n",
      "0.49301143583227447 388 787\n",
      "0.49238578680203043 388 788\n",
      "0.4917617237008872 388 789\n",
      "0.49240506329113926 389 790\n",
      "0.49304677623261695 390 791\n",
      "0.4936868686868687 391 792\n",
      "0.4930643127364439 391 793\n",
      "0.49370277078085645 392 794\n",
      "0.4930817610062893 392 795\n",
      "0.4937185929648241 393 796\n",
      "0.493099121706399 393 797\n",
      "0.49373433583959897 394 798\n",
      "0.493116395494368 394 799\n",
      "0.49375 395 800\n",
      "0.4943820224719101 396 801\n",
      "0.4937655860349127 396 802\n",
      "0.4931506849315068 396 803\n",
      "0.4937810945273632 397 804\n",
      "0.49440993788819876 398 805\n",
      "0.49379652605459057 398 806\n",
      "0.49318463444857497 398 807\n",
      "0.49257425742574257 398 808\n",
      "0.4932014833127318 399 809\n",
      "0.4925925925925926 399 810\n",
      "0.4932182490752158 400 811\n",
      "0.49261083743842365 400 812\n",
      "0.4920049200492005 400 813\n",
      "0.4914004914004914 400 814\n",
      "0.4920245398773006 401 815\n",
      "0.49264705882352944 402 816\n",
      "0.4920440636474908 402 817\n",
      "0.49266503667481665 403 818\n",
      "0.49206349206349204 403 819\n",
      "0.4926829268292683 404 820\n",
      "0.4933008526187576 405 821\n",
      "0.49391727493917276 406 822\n",
      "0.4933171324422843 406 823\n",
      "0.49393203883495146 407 824\n",
      "0.49454545454545457 408 825\n",
      "0.4939467312348668 408 826\n",
      "0.49334945586457074 408 827\n",
      "0.4927536231884058 408 828\n",
      "0.49215922798552475 408 829\n",
      "0.491566265060241 408 830\n",
      "0.49217809867629364 409 831\n",
      "0.49278846153846156 410 832\n",
      "0.4921968787515006 410 833\n",
      "0.49160671462829736 410 834\n",
      "0.49101796407185627 410 835\n",
      "0.4916267942583732 411 836\n",
      "0.4922341696535245 412 837\n",
      "0.4928400954653938 413 838\n",
      "0.4934445768772348 414 839\n",
      "0.4928571428571429 414 840\n",
      "0.49227110582639716 414 841\n",
      "0.49287410926365793 415 842\n",
      "0.49228944246737844 415 843\n",
      "0.4928909952606635 416 844\n",
      "0.493491124260355 417 845\n",
      "0.4929078014184397 417 846\n",
      "0.4935064935064935 418 847\n",
      "0.49410377358490565 419 848\n",
      "0.49469964664310956 420 849\n",
      "0.49411764705882355 420 850\n",
      "0.49471210340775557 421 851\n",
      "0.49413145539906106 421 852\n",
      "0.4935521688159437 421 853\n",
      "0.49414519906323184 422 854\n",
      "0.49473684210526314 423 855\n",
      "0.4941588785046729 423 856\n",
      "0.4935822637106184 423 857\n",
      "0.493006993006993 423 858\n",
      "0.4935972060535506 424 859\n",
      "0.4930232558139535 424 860\n",
      "0.4924506387921022 424 861\n",
      "0.49303944315545245 425 862\n",
      "0.4936268829663963 426 863\n",
      "0.4930555555555556 426 864\n",
      "0.4936416184971098 427 865\n",
      "0.4930715935334873 427 866\n",
      "0.4925028835063437 427 867\n",
      "0.49193548387096775 427 868\n",
      "0.49136939010356734 427 869\n",
      "0.4908045977011494 427 870\n",
      "0.4902411021814007 427 871\n",
      "0.4908256880733945 428 872\n",
      "0.49026345933562426 428 873\n",
      "0.49084668192219677 429 874\n",
      "0.49142857142857144 430 875\n",
      "0.4908675799086758 430 876\n",
      "0.4903078677309008 430 877\n",
      "0.489749430523918 430 878\n",
      "0.4891922639362912 430 879\n",
      "0.48863636363636365 430 880\n",
      "0.4880817253121453 430 881\n",
      "0.4875283446712018 430 882\n",
      "0.4869762174405436 430 883\n",
      "0.48755656108597284 431 884\n",
      "0.48700564971751414 431 885\n",
      "0.48758465011286684 432 886\n",
      "0.4881623449830891 433 887\n",
      "0.48873873873873874 434 888\n",
      "0.4881889763779528 434 889\n",
      "0.4887640449438202 435 890\n",
      "0.489337822671156 436 891\n",
      "0.4899103139013453 437 892\n",
      "0.48936170212765956 437 893\n",
      "0.4899328859060403 438 894\n",
      "0.49050279329608937 439 895\n",
      "0.49107142857142855 440 896\n",
      "0.4916387959866221 441 897\n",
      "0.4922048997772829 442 898\n",
      "0.4916573971078977 442 899\n",
      "0.4922222222222222 443 900\n",
      "0.49278579356270813 444 901\n",
      "0.49223946784922396 444 902\n",
      "0.49280177187153934 445 903\n",
      "0.49336283185840707 446 904\n",
      "0.49392265193370166 447 905\n",
      "0.49337748344370863 447 906\n",
      "0.49393605292171994 448 907\n",
      "0.4944933920704846 449 908\n",
      "0.49394939493949397 449 909\n",
      "0.4945054945054945 450 910\n",
      "0.4950603732162459 451 911\n",
      "0.4956140350877193 452 912\n",
      "0.4950711938663746 452 913\n",
      "0.49452954048140046 452 914\n",
      "0.49508196721311476 453 915\n",
      "0.4945414847161572 453 916\n",
      "0.495092693565976 454 917\n",
      "0.49564270152505446 455 918\n",
      "0.49510337323177367 455 919\n",
      "0.4956521739130435 456 920\n",
      "0.496199782844734 457 921\n",
      "0.4967462039045553 458 922\n",
      "0.4962080173347779 458 923\n",
      "0.49567099567099565 458 924\n",
      "0.49513513513513513 458 925\n",
      "0.4946004319654428 458 926\n",
      "0.49406688241639696 458 927\n",
      "0.49461206896551724 459 928\n",
      "0.4940796555435953 459 929\n",
      "0.4935483870967742 459 930\n",
      "0.4940923737916219 460 931\n",
      "0.49356223175965663 460 932\n",
      "0.4930332261521972 460 933\n",
      "0.493576017130621 461 934\n",
      "0.49411764705882355 462 935\n",
      "0.4946581196581197 463 936\n",
      "0.4951974386339381 464 937\n",
      "0.4946695095948827 464 938\n",
      "0.4941427050053248 464 939\n",
      "0.4946808510638298 465 940\n",
      "0.4952178533475027 466 941\n",
      "0.49469214437367304 466 942\n",
      "0.49522799575821846 467 943\n",
      "0.4947033898305085 467 944\n",
      "0.49417989417989416 467 945\n",
      "0.49365750528541225 467 946\n",
      "0.4931362196409715 467 947\n",
      "0.4936708860759494 468 948\n",
      "0.49420442571127504 469 949\n",
      "0.4936842105263158 469 950\n",
      "0.4942166140904311 470 951\n",
      "0.4947478991596639 471 952\n",
      "0.49527806925498424 472 953\n",
      "0.4947589098532495 472 954\n",
      "0.4942408376963351 472 955\n",
      "0.49372384937238495 472 956\n",
      "0.49320794148380354 472 957\n",
      "0.49373695198329853 473 958\n",
      "0.4932221063607925 473 959\n",
      "0.49375 474 960\n",
      "0.49323621227887615 474 961\n",
      "0.49376299376299376 475 962\n",
      "0.4932502596053998 475 963\n",
      "0.49377593360995853 476 964\n",
      "0.49430051813471504 477 965\n",
      "0.494824016563147 478 966\n",
      "0.4953464322647363 479 967\n",
      "0.49586776859504134 480 968\n",
      "0.4953560371517028 480 969\n",
      "0.49587628865979383 481 970\n",
      "0.49536560247167866 481 971\n",
      "0.4948559670781893 481 972\n",
      "0.49537512846865367 482 973\n",
      "0.4948665297741273 482 974\n",
      "0.49435897435897436 482 975\n",
      "0.4948770491803279 483 976\n",
      "0.49437052200614123 483 977\n",
      "0.4938650306748466 483 978\n",
      "0.4933605720122574 483 979\n",
      "0.49387755102040815 484 980\n",
      "0.49337410805300713 484 981\n",
      "0.49287169042769857 484 982\n",
      "0.4923702950152594 484 983\n",
      "0.491869918699187 484 984\n",
      "0.49238578680203043 485 985\n",
      "0.4918864097363083 485 986\n",
      "0.49240121580547114 486 987\n",
      "0.49291497975708504 487 988\n",
      "0.49342770475227504 488 989\n",
      "0.49292929292929294 488 990\n",
      "0.4924318869828456 488 991\n",
      "0.49294354838709675 489 992\n",
      "0.49244712990936557 489 993\n",
      "0.4919517102615694 489 994\n",
      "0.4914572864321608 489 995\n",
      "0.49096385542168675 489 996\n",
      "0.49147442326980945 490 997\n",
      "0.49198396793587174 491 998\n",
      "0.4914914914914915 491 999\n",
      "0.491 491 1000\n",
      "run 12740 practice games out of an allotted 100,000. practice success rate so far = 0.491\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    api.start_game(practice=1,verbose=False)\n",
    "    [total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "    practice_success_rate = (total_practice_successes - prev_successes)/ (total_practice_runs-prev_runs)\n",
    "    print(practice_success_rate,(total_practice_successes - prev_successes),(total_practice_runs-prev_runs))\n",
    "    time.sleep(0.5)\n",
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "practice_success_rate = (total_practice_successes - prev_successes)/ (total_practice_runs-prev_runs)\n",
    "print('run %d practice games out of an allotted 100,000. practice success rate so far = %.3f' % (total_practice_runs, practice_success_rate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.491, 491, 1000)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practice_success_rate,(total_practice_successes - prev_successes),(total_practice_runs-prev_runs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its not >=50% but I hope it is good enough. Thank you "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing recorded games:\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12760, 5701)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_runs,_,_,prev_successes=api.my_status()\n",
    "prev_runs,prev_successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing  0  th game\n",
      "Playing  1  th game\n",
      "Playing  2  th game\n",
      "Playing  3  th game\n",
      "Playing  4  th game\n",
      "Playing  5  th game\n",
      "Playing  6  th game\n",
      "Playing  7  th game\n",
      "Playing  8  th game\n",
      "Playing  9  th game\n",
      "Playing  10  th game\n",
      "Playing  11  th game\n",
      "Playing  12  th game\n",
      "Playing  13  th game\n",
      "Playing  14  th game\n",
      "Playing  15  th game\n",
      "Playing  16  th game\n",
      "Playing  17  th game\n",
      "Playing  18  th game\n",
      "Playing  19  th game\n",
      "Playing  20  th game\n",
      "Playing  21  th game\n",
      "Playing  22  th game\n",
      "Playing  23  th game\n",
      "Playing  24  th game\n",
      "Playing  25  th game\n",
      "Playing  26  th game\n",
      "Playing  27  th game\n",
      "Playing  28  th game\n",
      "Playing  29  th game\n",
      "Playing  30  th game\n",
      "Playing  31  th game\n",
      "Playing  32  th game\n",
      "Playing  33  th game\n",
      "Playing  34  th game\n",
      "Playing  35  th game\n",
      "Playing  36  th game\n",
      "Playing  37  th game\n",
      "Playing  38  th game\n",
      "Playing  39  th game\n",
      "Playing  40  th game\n",
      "Playing  41  th game\n",
      "Playing  42  th game\n",
      "Playing  43  th game\n",
      "Playing  44  th game\n",
      "Playing  45  th game\n",
      "Playing  46  th game\n",
      "Playing  47  th game\n",
      "Playing  48  th game\n",
      "Playing  49  th game\n",
      "Playing  50  th game\n",
      "Playing  51  th game\n",
      "Playing  52  th game\n",
      "Playing  53  th game\n",
      "Playing  54  th game\n",
      "Playing  55  th game\n",
      "Playing  56  th game\n",
      "Playing  57  th game\n",
      "Playing  58  th game\n",
      "Playing  59  th game\n",
      "Playing  60  th game\n",
      "Playing  61  th game\n",
      "Playing  62  th game\n",
      "Playing  63  th game\n",
      "Playing  64  th game\n",
      "Playing  65  th game\n",
      "Playing  66  th game\n",
      "Playing  67  th game\n",
      "Playing  68  th game\n",
      "Playing  69  th game\n",
      "Playing  70  th game\n",
      "Playing  71  th game\n",
      "Playing  72  th game\n",
      "Playing  73  th game\n",
      "Playing  74  th game\n",
      "Playing  75  th game\n",
      "Playing  76  th game\n",
      "Playing  77  th game\n",
      "Playing  78  th game\n",
      "Playing  79  th game\n",
      "Playing  80  th game\n",
      "Playing  81  th game\n",
      "Playing  82  th game\n",
      "Playing  83  th game\n",
      "Playing  84  th game\n",
      "Playing  85  th game\n",
      "Playing  86  th game\n",
      "Playing  87  th game\n",
      "Playing  88  th game\n",
      "Playing  89  th game\n",
      "Playing  90  th game\n",
      "Playing  91  th game\n",
      "Playing  92  th game\n",
      "Playing  93  th game\n",
      "Playing  94  th game\n",
      "Playing  95  th game\n",
      "Playing  96  th game\n",
      "Playing  97  th game\n",
      "Playing  98  th game\n",
      "Playing  99  th game\n",
      "Playing  100  th game\n",
      "Playing  101  th game\n",
      "Playing  102  th game\n",
      "Playing  103  th game\n",
      "Playing  104  th game\n",
      "Playing  105  th game\n",
      "Playing  106  th game\n",
      "Playing  107  th game\n",
      "Playing  108  th game\n",
      "Playing  109  th game\n",
      "Playing  110  th game\n",
      "Playing  111  th game\n",
      "Playing  112  th game\n",
      "Playing  113  th game\n",
      "Playing  114  th game\n",
      "Playing  115  th game\n",
      "Playing  116  th game\n",
      "Playing  117  th game\n",
      "Playing  118  th game\n",
      "Playing  119  th game\n",
      "Playing  120  th game\n",
      "Playing  121  th game\n",
      "Playing  122  th game\n",
      "Playing  123  th game\n",
      "Playing  124  th game\n",
      "Playing  125  th game\n",
      "Playing  126  th game\n",
      "Playing  127  th game\n",
      "Playing  128  th game\n",
      "Playing  129  th game\n",
      "Playing  130  th game\n",
      "Playing  131  th game\n",
      "Playing  132  th game\n",
      "Playing  133  th game\n",
      "Playing  134  th game\n",
      "Playing  135  th game\n",
      "Playing  136  th game\n",
      "Playing  137  th game\n",
      "Playing  138  th game\n",
      "Playing  139  th game\n",
      "Playing  140  th game\n",
      "Playing  141  th game\n",
      "Playing  142  th game\n",
      "Playing  143  th game\n",
      "Playing  144  th game\n",
      "Playing  145  th game\n",
      "Playing  146  th game\n",
      "Playing  147  th game\n",
      "Playing  148  th game\n",
      "Playing  149  th game\n",
      "Playing  150  th game\n",
      "Playing  151  th game\n",
      "Playing  152  th game\n",
      "Playing  153  th game\n",
      "Playing  154  th game\n",
      "Playing  155  th game\n",
      "Playing  156  th game\n",
      "Playing  157  th game\n",
      "Playing  158  th game\n",
      "Playing  159  th game\n",
      "Playing  160  th game\n",
      "Playing  161  th game\n",
      "Playing  162  th game\n",
      "Playing  163  th game\n",
      "Playing  164  th game\n",
      "Playing  165  th game\n",
      "Playing  166  th game\n",
      "Playing  167  th game\n",
      "Playing  168  th game\n",
      "Playing  169  th game\n",
      "Playing  170  th game\n",
      "Playing  171  th game\n",
      "Playing  172  th game\n",
      "Playing  173  th game\n",
      "Playing  174  th game\n",
      "Playing  175  th game\n",
      "Playing  176  th game\n",
      "Playing  177  th game\n",
      "Playing  178  th game\n",
      "Playing  179  th game\n",
      "Playing  180  th game\n",
      "Playing  181  th game\n",
      "Playing  182  th game\n",
      "Playing  183  th game\n",
      "Playing  184  th game\n",
      "Playing  185  th game\n",
      "Playing  186  th game\n",
      "Playing  187  th game\n",
      "Playing  188  th game\n",
      "Playing  189  th game\n",
      "Playing  190  th game\n",
      "Playing  191  th game\n",
      "Playing  192  th game\n",
      "Playing  193  th game\n",
      "Playing  194  th game\n",
      "Playing  195  th game\n",
      "Playing  196  th game\n",
      "Playing  197  th game\n",
      "Playing  198  th game\n",
      "Playing  199  th game\n",
      "Playing  200  th game\n",
      "Playing  201  th game\n",
      "Playing  202  th game\n",
      "Playing  203  th game\n",
      "Playing  204  th game\n",
      "Playing  205  th game\n",
      "Playing  206  th game\n",
      "Playing  207  th game\n",
      "Playing  208  th game\n",
      "Playing  209  th game\n",
      "Playing  210  th game\n",
      "Playing  211  th game\n",
      "Playing  212  th game\n",
      "Playing  213  th game\n",
      "Playing  214  th game\n",
      "Playing  215  th game\n",
      "Playing  216  th game\n",
      "Playing  217  th game\n",
      "Playing  218  th game\n",
      "Playing  219  th game\n",
      "Playing  220  th game\n",
      "Playing  221  th game\n",
      "Playing  222  th game\n",
      "Playing  223  th game\n",
      "Playing  224  th game\n",
      "Playing  225  th game\n",
      "Playing  226  th game\n",
      "Playing  227  th game\n",
      "Playing  228  th game\n",
      "Playing  229  th game\n",
      "Playing  230  th game\n",
      "Playing  231  th game\n",
      "Playing  232  th game\n",
      "Playing  233  th game\n",
      "Playing  234  th game\n",
      "Playing  235  th game\n",
      "Playing  236  th game\n",
      "Playing  237  th game\n",
      "Playing  238  th game\n",
      "Playing  239  th game\n",
      "Playing  240  th game\n",
      "Playing  241  th game\n",
      "Playing  242  th game\n",
      "Playing  243  th game\n",
      "Playing  244  th game\n",
      "Playing  245  th game\n",
      "Playing  246  th game\n",
      "Playing  247  th game\n",
      "Playing  248  th game\n",
      "Playing  249  th game\n",
      "Playing  250  th game\n",
      "Playing  251  th game\n",
      "Playing  252  th game\n",
      "Playing  253  th game\n",
      "Playing  254  th game\n",
      "Playing  255  th game\n",
      "Playing  256  th game\n",
      "Playing  257  th game\n",
      "Playing  258  th game\n",
      "Playing  259  th game\n",
      "Playing  260  th game\n",
      "Playing  261  th game\n",
      "Playing  262  th game\n",
      "Playing  263  th game\n",
      "Playing  264  th game\n",
      "Playing  265  th game\n",
      "Playing  266  th game\n",
      "Playing  267  th game\n",
      "Playing  268  th game\n",
      "Playing  269  th game\n",
      "Playing  270  th game\n",
      "Playing  271  th game\n",
      "Playing  272  th game\n",
      "Playing  273  th game\n",
      "Playing  274  th game\n",
      "Playing  275  th game\n",
      "Playing  276  th game\n",
      "Playing  277  th game\n",
      "Playing  278  th game\n",
      "Playing  279  th game\n",
      "Playing  280  th game\n",
      "Playing  281  th game\n",
      "Playing  282  th game\n",
      "Playing  283  th game\n",
      "Playing  284  th game\n",
      "Playing  285  th game\n",
      "Playing  286  th game\n",
      "Playing  287  th game\n",
      "Playing  288  th game\n",
      "Playing  289  th game\n",
      "Playing  290  th game\n",
      "Playing  291  th game\n",
      "Playing  292  th game\n",
      "Playing  293  th game\n",
      "Playing  294  th game\n",
      "Playing  295  th game\n",
      "Playing  296  th game\n",
      "Playing  297  th game\n",
      "Playing  298  th game\n",
      "Playing  299  th game\n",
      "Playing  300  th game\n",
      "Playing  301  th game\n",
      "Playing  302  th game\n",
      "Playing  303  th game\n",
      "Playing  304  th game\n",
      "Playing  305  th game\n",
      "Playing  306  th game\n",
      "Playing  307  th game\n",
      "Playing  308  th game\n",
      "Playing  309  th game\n",
      "Playing  310  th game\n",
      "Playing  311  th game\n",
      "Playing  312  th game\n",
      "Playing  313  th game\n",
      "Playing  314  th game\n",
      "Playing  315  th game\n",
      "Playing  316  th game\n",
      "Playing  317  th game\n",
      "Playing  318  th game\n",
      "Playing  319  th game\n",
      "Playing  320  th game\n",
      "Playing  321  th game\n",
      "Playing  322  th game\n",
      "Playing  323  th game\n",
      "Playing  324  th game\n",
      "Playing  325  th game\n",
      "Playing  326  th game\n",
      "Playing  327  th game\n",
      "Playing  328  th game\n",
      "Playing  329  th game\n",
      "Playing  330  th game\n",
      "Playing  331  th game\n",
      "Playing  332  th game\n",
      "Playing  333  th game\n",
      "Playing  334  th game\n",
      "Playing  335  th game\n",
      "Playing  336  th game\n",
      "Playing  337  th game\n",
      "Playing  338  th game\n",
      "Playing  339  th game\n",
      "Playing  340  th game\n",
      "Playing  341  th game\n",
      "Playing  342  th game\n",
      "Playing  343  th game\n",
      "Playing  344  th game\n",
      "Playing  345  th game\n",
      "Playing  346  th game\n",
      "Playing  347  th game\n",
      "Playing  348  th game\n",
      "Playing  349  th game\n",
      "Playing  350  th game\n",
      "Playing  351  th game\n",
      "Playing  352  th game\n",
      "Playing  353  th game\n",
      "Playing  354  th game\n",
      "Playing  355  th game\n",
      "Playing  356  th game\n",
      "Playing  357  th game\n",
      "Playing  358  th game\n",
      "Playing  359  th game\n",
      "Playing  360  th game\n",
      "Playing  361  th game\n",
      "Playing  362  th game\n",
      "Playing  363  th game\n",
      "Playing  364  th game\n",
      "Playing  365  th game\n",
      "Playing  366  th game\n",
      "Playing  367  th game\n",
      "Playing  368  th game\n",
      "Playing  369  th game\n",
      "Playing  370  th game\n",
      "Playing  371  th game\n",
      "Playing  372  th game\n",
      "Playing  373  th game\n",
      "Playing  374  th game\n",
      "Playing  375  th game\n",
      "Playing  376  th game\n",
      "Playing  377  th game\n",
      "Playing  378  th game\n",
      "Playing  379  th game\n",
      "Playing  380  th game\n",
      "Playing  381  th game\n",
      "Playing  382  th game\n",
      "Playing  383  th game\n",
      "Playing  384  th game\n",
      "Playing  385  th game\n",
      "Playing  386  th game\n",
      "Playing  387  th game\n",
      "Playing  388  th game\n",
      "Playing  389  th game\n",
      "Playing  390  th game\n",
      "Playing  391  th game\n",
      "Playing  392  th game\n",
      "Playing  393  th game\n",
      "Playing  394  th game\n",
      "Playing  395  th game\n",
      "Playing  396  th game\n",
      "Playing  397  th game\n",
      "Playing  398  th game\n",
      "Playing  399  th game\n",
      "Playing  400  th game\n",
      "Playing  401  th game\n",
      "Playing  402  th game\n",
      "Playing  403  th game\n",
      "Playing  404  th game\n",
      "Playing  405  th game\n",
      "Playing  406  th game\n",
      "Playing  407  th game\n",
      "Playing  408  th game\n",
      "Playing  409  th game\n",
      "Playing  410  th game\n",
      "Playing  411  th game\n",
      "Playing  412  th game\n",
      "Playing  413  th game\n",
      "Playing  414  th game\n",
      "Playing  415  th game\n",
      "Playing  416  th game\n",
      "Playing  417  th game\n",
      "Playing  418  th game\n",
      "Playing  419  th game\n",
      "Playing  420  th game\n",
      "Playing  421  th game\n",
      "Playing  422  th game\n",
      "Playing  423  th game\n",
      "Playing  424  th game\n",
      "Playing  425  th game\n",
      "Playing  426  th game\n",
      "Playing  427  th game\n",
      "Playing  428  th game\n",
      "Playing  429  th game\n",
      "Playing  430  th game\n",
      "Playing  431  th game\n",
      "Playing  432  th game\n",
      "Playing  433  th game\n",
      "Playing  434  th game\n",
      "Playing  435  th game\n",
      "Playing  436  th game\n",
      "Playing  437  th game\n",
      "Playing  438  th game\n",
      "Playing  439  th game\n",
      "Playing  440  th game\n",
      "Playing  441  th game\n",
      "Playing  442  th game\n",
      "Playing  443  th game\n",
      "Playing  444  th game\n",
      "Playing  445  th game\n",
      "Playing  446  th game\n",
      "Playing  447  th game\n",
      "Playing  448  th game\n",
      "Playing  449  th game\n",
      "Playing  450  th game\n",
      "Playing  451  th game\n",
      "Playing  452  th game\n",
      "Playing  453  th game\n",
      "Playing  454  th game\n",
      "Playing  455  th game\n",
      "Playing  456  th game\n",
      "Playing  457  th game\n",
      "Playing  458  th game\n",
      "Playing  459  th game\n",
      "Playing  460  th game\n",
      "Playing  461  th game\n",
      "Playing  462  th game\n",
      "Playing  463  th game\n",
      "Playing  464  th game\n",
      "Playing  465  th game\n",
      "Playing  466  th game\n",
      "Playing  467  th game\n",
      "Playing  468  th game\n",
      "Playing  469  th game\n",
      "Playing  470  th game\n",
      "Playing  471  th game\n",
      "Playing  472  th game\n",
      "Playing  473  th game\n",
      "Playing  474  th game\n",
      "Playing  475  th game\n",
      "Playing  476  th game\n",
      "Playing  477  th game\n",
      "Playing  478  th game\n",
      "Playing  479  th game\n",
      "Playing  480  th game\n",
      "Playing  481  th game\n",
      "Playing  482  th game\n",
      "Playing  483  th game\n",
      "Playing  484  th game\n",
      "Playing  485  th game\n",
      "Playing  486  th game\n",
      "Playing  487  th game\n",
      "Playing  488  th game\n",
      "Playing  489  th game\n",
      "Playing  490  th game\n",
      "Playing  491  th game\n",
      "Playing  492  th game\n",
      "Playing  493  th game\n",
      "Playing  494  th game\n",
      "Playing  495  th game\n",
      "Playing  496  th game\n",
      "Playing  497  th game\n",
      "Playing  498  th game\n",
      "Playing  499  th game\n",
      "Playing  500  th game\n",
      "Playing  501  th game\n",
      "Playing  502  th game\n",
      "Playing  503  th game\n",
      "Playing  504  th game\n",
      "Playing  505  th game\n",
      "Playing  506  th game\n",
      "Playing  507  th game\n",
      "Playing  508  th game\n",
      "Playing  509  th game\n",
      "Playing  510  th game\n",
      "Playing  511  th game\n",
      "Playing  512  th game\n",
      "Playing  513  th game\n",
      "Playing  514  th game\n",
      "Playing  515  th game\n",
      "Playing  516  th game\n",
      "Playing  517  th game\n",
      "Playing  518  th game\n",
      "Playing  519  th game\n",
      "Playing  520  th game\n",
      "Playing  521  th game\n",
      "Playing  522  th game\n",
      "Playing  523  th game\n",
      "Playing  524  th game\n",
      "Playing  525  th game\n",
      "Playing  526  th game\n",
      "Playing  527  th game\n",
      "Playing  528  th game\n",
      "Playing  529  th game\n",
      "Playing  530  th game\n",
      "Playing  531  th game\n",
      "Playing  532  th game\n",
      "Playing  533  th game\n",
      "Playing  534  th game\n",
      "Playing  535  th game\n",
      "Playing  536  th game\n",
      "Playing  537  th game\n",
      "Playing  538  th game\n",
      "Playing  539  th game\n",
      "Playing  540  th game\n",
      "Playing  541  th game\n",
      "Playing  542  th game\n",
      "Playing  543  th game\n",
      "Playing  544  th game\n",
      "Playing  545  th game\n",
      "Playing  546  th game\n",
      "Playing  547  th game\n",
      "Playing  548  th game\n",
      "Playing  549  th game\n",
      "Playing  550  th game\n",
      "Playing  551  th game\n",
      "Playing  552  th game\n",
      "Playing  553  th game\n",
      "Playing  554  th game\n",
      "Playing  555  th game\n",
      "Playing  556  th game\n",
      "Playing  557  th game\n",
      "Playing  558  th game\n",
      "Playing  559  th game\n",
      "Playing  560  th game\n",
      "Playing  561  th game\n",
      "Playing  562  th game\n",
      "Playing  563  th game\n",
      "Playing  564  th game\n",
      "Playing  565  th game\n",
      "Playing  566  th game\n",
      "Playing  567  th game\n",
      "Playing  568  th game\n",
      "Playing  569  th game\n",
      "Playing  570  th game\n",
      "Playing  571  th game\n",
      "Playing  572  th game\n",
      "Playing  573  th game\n",
      "Playing  574  th game\n",
      "Playing  575  th game\n",
      "Playing  576  th game\n",
      "Playing  577  th game\n",
      "Playing  578  th game\n",
      "Playing  579  th game\n",
      "Playing  580  th game\n",
      "Playing  581  th game\n",
      "Playing  582  th game\n",
      "Playing  583  th game\n",
      "Playing  584  th game\n",
      "Playing  585  th game\n",
      "Playing  586  th game\n",
      "Playing  587  th game\n",
      "Playing  588  th game\n",
      "Playing  589  th game\n",
      "Playing  590  th game\n",
      "Playing  591  th game\n",
      "Playing  592  th game\n",
      "Playing  593  th game\n",
      "Playing  594  th game\n",
      "Playing  595  th game\n",
      "Playing  596  th game\n",
      "Playing  597  th game\n",
      "Playing  598  th game\n",
      "Playing  599  th game\n",
      "Playing  600  th game\n",
      "Playing  601  th game\n",
      "Playing  602  th game\n",
      "Playing  603  th game\n",
      "Playing  604  th game\n",
      "Playing  605  th game\n",
      "Playing  606  th game\n",
      "Playing  607  th game\n",
      "Playing  608  th game\n",
      "Playing  609  th game\n",
      "Playing  610  th game\n",
      "Playing  611  th game\n",
      "Playing  612  th game\n",
      "Playing  613  th game\n",
      "Playing  614  th game\n",
      "Playing  615  th game\n",
      "Playing  616  th game\n",
      "Playing  617  th game\n",
      "Playing  618  th game\n",
      "Playing  619  th game\n",
      "Playing  620  th game\n",
      "Playing  621  th game\n",
      "Playing  622  th game\n",
      "Playing  623  th game\n",
      "Playing  624  th game\n",
      "Playing  625  th game\n",
      "Playing  626  th game\n",
      "Playing  627  th game\n",
      "Playing  628  th game\n",
      "Playing  629  th game\n",
      "Playing  630  th game\n",
      "Playing  631  th game\n",
      "Playing  632  th game\n",
      "Playing  633  th game\n",
      "Playing  634  th game\n",
      "Playing  635  th game\n",
      "Playing  636  th game\n",
      "Playing  637  th game\n",
      "Playing  638  th game\n",
      "Playing  639  th game\n",
      "Playing  640  th game\n",
      "Playing  641  th game\n",
      "Playing  642  th game\n",
      "Playing  643  th game\n",
      "Playing  644  th game\n",
      "Playing  645  th game\n",
      "Playing  646  th game\n",
      "Playing  647  th game\n",
      "Playing  648  th game\n",
      "Playing  649  th game\n",
      "Playing  650  th game\n",
      "Playing  651  th game\n",
      "Playing  652  th game\n",
      "Playing  653  th game\n",
      "Playing  654  th game\n",
      "Playing  655  th game\n",
      "Playing  656  th game\n",
      "Playing  657  th game\n",
      "Playing  658  th game\n",
      "Playing  659  th game\n",
      "Playing  660  th game\n",
      "Playing  661  th game\n",
      "Playing  662  th game\n",
      "Playing  663  th game\n",
      "Playing  664  th game\n",
      "Playing  665  th game\n",
      "Playing  666  th game\n",
      "Playing  667  th game\n",
      "Playing  668  th game\n",
      "Playing  669  th game\n",
      "Playing  670  th game\n",
      "Playing  671  th game\n",
      "Playing  672  th game\n",
      "Playing  673  th game\n",
      "Playing  674  th game\n",
      "Playing  675  th game\n",
      "Playing  676  th game\n",
      "Playing  677  th game\n",
      "Playing  678  th game\n",
      "Playing  679  th game\n",
      "Playing  680  th game\n",
      "Playing  681  th game\n",
      "Playing  682  th game\n",
      "Playing  683  th game\n",
      "Playing  684  th game\n",
      "Playing  685  th game\n",
      "Playing  686  th game\n",
      "Playing  687  th game\n",
      "Playing  688  th game\n",
      "Playing  689  th game\n",
      "Playing  690  th game\n",
      "Playing  691  th game\n",
      "Playing  692  th game\n",
      "Playing  693  th game\n",
      "Playing  694  th game\n",
      "Playing  695  th game\n",
      "Playing  696  th game\n",
      "Playing  697  th game\n",
      "Playing  698  th game\n",
      "Playing  699  th game\n",
      "Playing  700  th game\n",
      "Playing  701  th game\n",
      "Playing  702  th game\n",
      "Playing  703  th game\n",
      "Playing  704  th game\n",
      "Playing  705  th game\n",
      "Playing  706  th game\n",
      "Playing  707  th game\n",
      "Playing  708  th game\n",
      "Playing  709  th game\n",
      "Playing  710  th game\n",
      "Playing  711  th game\n",
      "Playing  712  th game\n",
      "Playing  713  th game\n",
      "Playing  714  th game\n",
      "Playing  715  th game\n",
      "Playing  716  th game\n",
      "Playing  717  th game\n",
      "Playing  718  th game\n",
      "Playing  719  th game\n",
      "Playing  720  th game\n",
      "Playing  721  th game\n",
      "Playing  722  th game\n",
      "Playing  723  th game\n",
      "Playing  724  th game\n",
      "Playing  725  th game\n",
      "Playing  726  th game\n",
      "Playing  727  th game\n",
      "Playing  728  th game\n",
      "Playing  729  th game\n",
      "Playing  730  th game\n",
      "Playing  731  th game\n",
      "Playing  732  th game\n",
      "Playing  733  th game\n",
      "Playing  734  th game\n",
      "Playing  735  th game\n",
      "Playing  736  th game\n",
      "Playing  737  th game\n",
      "Playing  738  th game\n",
      "Playing  739  th game\n",
      "Playing  740  th game\n",
      "Playing  741  th game\n",
      "Playing  742  th game\n",
      "Playing  743  th game\n",
      "Playing  744  th game\n",
      "Playing  745  th game\n",
      "Playing  746  th game\n",
      "Playing  747  th game\n",
      "Playing  748  th game\n",
      "Playing  749  th game\n",
      "Playing  750  th game\n",
      "Playing  751  th game\n",
      "Playing  752  th game\n",
      "Playing  753  th game\n",
      "Playing  754  th game\n",
      "Playing  755  th game\n",
      "Playing  756  th game\n",
      "Playing  757  th game\n",
      "Playing  758  th game\n",
      "Playing  759  th game\n",
      "Playing  760  th game\n",
      "Playing  761  th game\n",
      "Playing  762  th game\n",
      "Playing  763  th game\n",
      "Playing  764  th game\n",
      "Playing  765  th game\n",
      "Playing  766  th game\n",
      "Playing  767  th game\n",
      "Playing  768  th game\n",
      "Playing  769  th game\n",
      "Playing  770  th game\n",
      "Playing  771  th game\n",
      "Playing  772  th game\n",
      "Playing  773  th game\n",
      "Playing  774  th game\n",
      "Playing  775  th game\n",
      "Playing  776  th game\n",
      "Playing  777  th game\n",
      "Playing  778  th game\n",
      "Playing  779  th game\n",
      "Playing  780  th game\n",
      "Playing  781  th game\n",
      "Playing  782  th game\n",
      "Playing  783  th game\n",
      "Playing  784  th game\n",
      "Playing  785  th game\n",
      "Playing  786  th game\n",
      "Playing  787  th game\n",
      "Playing  788  th game\n",
      "Playing  789  th game\n",
      "Playing  790  th game\n",
      "Playing  791  th game\n",
      "Playing  792  th game\n",
      "Playing  793  th game\n",
      "Playing  794  th game\n",
      "Playing  795  th game\n",
      "Playing  796  th game\n",
      "Playing  797  th game\n",
      "Playing  798  th game\n",
      "Playing  799  th game\n",
      "Playing  800  th game\n",
      "Playing  801  th game\n",
      "Playing  802  th game\n",
      "Playing  803  th game\n",
      "Playing  804  th game\n",
      "Playing  805  th game\n",
      "Playing  806  th game\n",
      "Playing  807  th game\n",
      "Playing  808  th game\n",
      "Playing  809  th game\n",
      "Playing  810  th game\n",
      "Playing  811  th game\n",
      "Playing  812  th game\n",
      "Playing  813  th game\n",
      "Playing  814  th game\n",
      "Playing  815  th game\n",
      "Playing  816  th game\n",
      "Playing  817  th game\n",
      "Playing  818  th game\n",
      "Playing  819  th game\n",
      "Playing  820  th game\n",
      "Playing  821  th game\n",
      "Playing  822  th game\n",
      "Playing  823  th game\n",
      "Playing  824  th game\n",
      "Playing  825  th game\n",
      "Playing  826  th game\n",
      "Playing  827  th game\n",
      "Playing  828  th game\n",
      "Playing  829  th game\n",
      "Playing  830  th game\n",
      "Playing  831  th game\n",
      "Playing  832  th game\n",
      "Playing  833  th game\n",
      "Playing  834  th game\n",
      "Playing  835  th game\n",
      "Playing  836  th game\n",
      "Playing  837  th game\n",
      "Playing  838  th game\n",
      "Playing  839  th game\n",
      "Playing  840  th game\n",
      "Playing  841  th game\n",
      "Playing  842  th game\n",
      "Playing  843  th game\n",
      "Playing  844  th game\n",
      "Playing  845  th game\n",
      "Playing  846  th game\n",
      "Playing  847  th game\n",
      "Playing  848  th game\n",
      "Playing  849  th game\n",
      "Playing  850  th game\n",
      "Playing  851  th game\n",
      "Playing  852  th game\n",
      "Playing  853  th game\n",
      "Playing  854  th game\n",
      "Playing  855  th game\n",
      "Playing  856  th game\n",
      "Playing  857  th game\n",
      "Playing  858  th game\n",
      "Playing  859  th game\n",
      "Playing  860  th game\n",
      "Playing  861  th game\n",
      "Playing  862  th game\n",
      "Playing  863  th game\n",
      "Playing  864  th game\n",
      "Playing  865  th game\n",
      "Playing  866  th game\n",
      "Playing  867  th game\n",
      "Playing  868  th game\n",
      "Playing  869  th game\n",
      "Playing  870  th game\n",
      "Playing  871  th game\n",
      "Playing  872  th game\n",
      "Playing  873  th game\n",
      "Playing  874  th game\n",
      "Playing  875  th game\n",
      "Playing  876  th game\n",
      "Playing  877  th game\n",
      "Playing  878  th game\n",
      "Playing  879  th game\n",
      "Playing  880  th game\n",
      "Playing  881  th game\n",
      "Playing  882  th game\n",
      "Playing  883  th game\n",
      "Playing  884  th game\n",
      "Playing  885  th game\n",
      "Playing  886  th game\n",
      "Playing  887  th game\n",
      "Playing  888  th game\n",
      "Playing  889  th game\n",
      "Playing  890  th game\n",
      "Playing  891  th game\n",
      "Playing  892  th game\n",
      "Playing  893  th game\n",
      "Playing  894  th game\n",
      "Playing  895  th game\n",
      "Playing  896  th game\n",
      "Playing  897  th game\n",
      "Playing  898  th game\n",
      "Playing  899  th game\n",
      "Playing  900  th game\n",
      "Playing  901  th game\n",
      "Playing  902  th game\n",
      "Playing  903  th game\n",
      "Playing  904  th game\n",
      "Playing  905  th game\n",
      "Playing  906  th game\n",
      "Playing  907  th game\n",
      "Playing  908  th game\n",
      "Playing  909  th game\n",
      "Playing  910  th game\n",
      "Playing  911  th game\n",
      "Playing  912  th game\n",
      "Playing  913  th game\n",
      "Playing  914  th game\n",
      "Playing  915  th game\n",
      "Playing  916  th game\n",
      "Playing  917  th game\n",
      "Playing  918  th game\n",
      "Playing  919  th game\n",
      "Playing  920  th game\n",
      "Playing  921  th game\n",
      "Playing  922  th game\n",
      "Playing  923  th game\n",
      "Playing  924  th game\n",
      "Playing  925  th game\n",
      "Playing  926  th game\n",
      "Playing  927  th game\n",
      "Playing  928  th game\n",
      "Playing  929  th game\n",
      "Playing  930  th game\n",
      "Playing  931  th game\n",
      "Playing  932  th game\n",
      "Playing  933  th game\n",
      "Playing  934  th game\n",
      "Playing  935  th game\n",
      "Playing  936  th game\n",
      "Playing  937  th game\n",
      "Playing  938  th game\n",
      "Playing  939  th game\n",
      "Playing  940  th game\n",
      "Playing  941  th game\n",
      "Playing  942  th game\n",
      "Playing  943  th game\n",
      "Playing  944  th game\n",
      "Playing  945  th game\n",
      "Playing  946  th game\n",
      "Playing  947  th game\n",
      "Playing  948  th game\n",
      "Playing  949  th game\n",
      "Playing  950  th game\n",
      "Playing  951  th game\n",
      "Playing  952  th game\n",
      "Playing  953  th game\n",
      "Playing  954  th game\n",
      "Playing  955  th game\n",
      "Playing  956  th game\n",
      "Playing  957  th game\n",
      "Playing  958  th game\n",
      "Playing  959  th game\n",
      "Playing  960  th game\n",
      "Playing  961  th game\n",
      "Playing  962  th game\n",
      "Playing  963  th game\n",
      "Playing  964  th game\n",
      "Playing  965  th game\n",
      "Playing  966  th game\n",
      "Playing  967  th game\n",
      "Playing  968  th game\n",
      "Playing  969  th game\n",
      "Playing  970  th game\n",
      "Playing  971  th game\n",
      "Playing  972  th game\n",
      "Playing  973  th game\n",
      "Playing  974  th game\n",
      "Playing  975  th game\n",
      "Playing  976  th game\n",
      "Playing  977  th game\n",
      "Playing  978  th game\n",
      "Playing  979  th game\n",
      "Playing  980  th game\n",
      "Playing  981  th game\n",
      "Playing  982  th game\n",
      "Playing  983  th game\n",
      "Playing  984  th game\n",
      "Playing  985  th game\n",
      "Playing  986  th game\n",
      "Playing  987  th game\n",
      "Playing  988  th game\n",
      "Playing  989  th game\n",
      "Playing  990  th game\n",
      "Playing  991  th game\n",
      "Playing  992  th game\n",
      "Playing  993  th game\n",
      "Playing  994  th game\n",
      "Playing  995  th game\n",
      "Playing  996  th game\n",
      "Playing  997  th game\n",
      "Playing  998  th game\n"
     ]
    },
    {
     "ename": "HangmanAPIError",
     "evalue": "{'error': 'You have reached 1000 of games', 'status': 'denied'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlaying \u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m th game\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpractice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\u001b[39;00m\n\u001b[1;32m      7\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[152], line 102\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[0;34m(self, practice, verbose)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguessed_letters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_dictionary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_dictionary\n\u001b[0;32m--> 102\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/new_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpractice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpractice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    104\u001b[0m     game_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[152], line 204\u001b[0m, in \u001b[0;36mHangmanAPI.request\u001b[0;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintype was not text, or querystring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 204\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(result)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mHangmanAPIError\u001b[0m: {'error': 'You have reached 1000 of games', 'status': 'denied'}"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', i, ' th game')\n",
    "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
    "    api.start_game(practice=0,verbose=False)\n",
    "    \n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check your game statistics\n",
    "1. Simply use \"my_status\" method.\n",
    "2. Returns your total number of games, and number of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall success rate = 0.466\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/total_recorded_runs\n",
    "print('overall success rate = %.3f' % success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0, 0)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "practice_success_rate,(total_practice_successes - prev_successes),(total_practice_runs-prev_runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/senai2/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:546: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (Embedding): Embedding(30, 100, padding_idx=1)\n",
       "  (LSTM): LSTM(100, 300, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (convert): Linear(in_features=1800, out_features=100, bias=True)\n",
       "  (lin_layers): ModuleList()\n",
       "  (outputs): Linear(in_features=100, out_features=26, bias=True)\n",
       "  (act): SELU()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model=torch.load(\"MODEL2.pt\")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (Embedding): Embedding(30, 100, padding_idx=1)\n",
       "  (LSTM): LSTM(100, 300, num_layers=3, batch_first=True, dropout=0.4, bidirectional=True)\n",
       "  (convert): Linear(in_features=1800, out_features=100, bias=True)\n",
       "  (lin_layers): ModuleList()\n",
       "  (outputs): Linear(in_features=100, out_features=26, bias=True)\n",
       "  (act): SELU()\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
